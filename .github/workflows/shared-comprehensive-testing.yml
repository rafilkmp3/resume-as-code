---
name: üß™ Shared Comprehensive Testing Suite

on:
  workflow_call:
    inputs:
      target_url:
        description: 'URL to test (any environment)'
        required: true
        type: string
      environment_name:
        description: 'Environment name (preview, staging, production)'
        required: true
        type: string
      environment_context:
        description: 'Additional context (PR number, release tag, etc.)'
        required: false
        type: string
        default: ''
      test_types:
        description: 'Comma-separated test types to run (lighthouse,accessibility,security,performance)'
        required: false
        type: string
        default: 'lighthouse,accessibility,performance'
      lighthouse_budget_path:
        description: 'Path to Lighthouse budget.json file'
        required: false
        type: string
        default: './budget.json'
      artifact_retention_days:
        description: 'How many days to retain test artifacts'
        required: false
        type: number
        default: 7
      post_pr_comment:
        description: 'Whether to post results to PR comment (only works for PR context)'
        required: false
        type: boolean
        default: false
      pr_number:
        description: 'PR number for comment posting (required if post_pr_comment is true)'
        required: false
        type: string
        default: ''

permissions:
  contents: read
  pull-requests: write
  checks: write

jobs:
  # Environment-Agnostic Lighthouse Testing
  lighthouse-testing:
    name: üöÄ Lighthouse (${{ inputs.environment_name }})
    runs-on: ubuntu-latest
    if: contains(inputs.test_types, 'lighthouse')
    timeout-minutes: 10
    outputs:
      performance_score: ${{ steps.lighthouse-results.outputs.performance_score }}
      accessibility_score: ${{ steps.lighthouse-results.outputs.accessibility_score }}
      best_practices_score: ${{ steps.lighthouse-results.outputs.best_practices_score }}
      seo_score: ${{ steps.lighthouse-results.outputs.seo_score }}
      fcp: ${{ steps.lighthouse-results.outputs.fcp }}
      lcp: ${{ steps.lighthouse-results.outputs.lcp }}
      cls: ${{ steps.lighthouse-results.outputs.cls }}
      speed_index: ${{ steps.lighthouse-results.outputs.speed_index }}
      has_detailed_metrics: ${{ steps.lighthouse-results.outputs.has_detailed_metrics }}
      public_report_links: ${{ steps.lighthouse-results.outputs.public_report_links }}
      has_public_urls: ${{ steps.lighthouse-results.outputs.has_public_urls }}
      assertion_results: ${{ steps.lighthouse-results.outputs.assertion_results }}
      has_assertion_results: ${{ steps.lighthouse-results.outputs.has_assertion_results }}
      lighthouse_manifest: ${{ steps.lighthouse-results.outputs.lighthouse_manifest }}
      has_manifest: ${{ steps.lighthouse-results.outputs.has_manifest }}
    steps:
      - name: Checkout for budget configuration
        uses: actions/checkout@v5
        with:
          sparse-checkout: |
            budget.json
            
      - name: Environment-Agnostic Lighthouse Testing
        run: |
          echo "üöÄ LIGHTHOUSE TESTING - UNIVERSAL URL APPROACH"
          echo "=============================================="
          echo "üéØ TARGET URL: ${{ inputs.target_url }}"
          echo "üåç ENVIRONMENT: ${{ inputs.environment_name }}"
          echo "üìä CONTEXT: ${{ inputs.environment_context }}"
          echo "‚úÖ METHODOLOGY: Google Lighthouse CI with performance budgets"
          echo "üîó ADVANTAGE: Tests real user experience on any deployment"
          echo ""
          
          # Create dynamic Lighthouse config for ANY environment
          cat > .lighthouserc.json << 'EOF'
          {
            "ci": {
              "collect": {
                "url": ["${{ inputs.target_url }}"],
                "numberOfRuns": 3,
                "settings": {
                  "preset": "desktop",
                  "onlyCategories": ["performance", "accessibility", "best-practices", "seo"],
                  "chromeFlags": "--no-sandbox --headless=new --disable-gpu --disable-dev-shm-usage --disable-extensions --no-first-run --disable-default-apps --disable-background-timer-throttling --disable-backgrounding-occluded-windows --disable-renderer-backgrounding",
                  "maxWaitForLoad": 60000,
                  "maxWaitForFcp": 20000,
                  "skipAudits": ["uses-http2"],
                  "throttling": {
                    "rttMs": 40,
                    "throughputKbps": 10240,
                    "cpuSlowdownMultiplier": 1,
                    "requestLatencyMs": 0,
                    "downloadThroughputKbps": 0,
                    "uploadThroughputKbps": 0
                  }
                }
              },
              "assert": {
                "budgetPath": "${{ inputs.lighthouse_budget_path }}",
                "assertions": {
                  "categories:performance": ["warn", {"minScore": 0.85}],
                  "categories:accessibility": ["warn", {"minScore": 0.90}],
                  "categories:best-practices": ["warn", {"minScore": 0.85}],
                  "categories:seo": ["warn", {"minScore": 0.85}],
                  "categories:pwa": "off",
                  "first-contentful-paint": ["warn", {"maxNumericValue": 2000}],
                  "largest-contentful-paint": ["warn", {"maxNumericValue": 2500}],
                  "cumulative-layout-shift": ["warn", {"maxNumericValue": 0.1}],
                  "speed-index": ["warn", {"maxNumericValue": 3000}]
                }
              },
              "upload": {
                "target": "temporary-public-storage",
                "temporaryPublicStorage": true
              }
            }
          }
          EOF

      - name: Execute Lighthouse CI
        uses: treosh/lighthouse-ci-action@v12
        with:
          configPath: '.lighthouserc.json'
          uploadArtifacts: true
        continue-on-error: true
        id: lighthouse-run

      - name: Extract Lighthouse Metrics
        id: lighthouse-results
        run: |
          echo "üìä EXTRACTING LIGHTHOUSE METRICS FOR ${{ inputs.environment_name }}"
          
          # Install jq for JSON parsing
          if ! command -v jq >/dev/null 2>&1; then
            sudo apt-get update && sudo apt-get install -y jq
          fi
          
          # EXTRACT TEMPORARY PUBLIC STORAGE URLS AND ASSERTION RESULTS
          echo "üîó CHECKING FOR TEMPORARY PUBLIC STORAGE URLS:"
          if [ -n "${{ steps.lighthouse-run.outputs.links }}" ]; then
            echo "‚úÖ Found public report links: ${{ steps.lighthouse-run.outputs.links }}"
            echo "public_report_links=${{ steps.lighthouse-run.outputs.links }}" >> $GITHUB_OUTPUT
            echo "has_public_urls=true" >> $GITHUB_OUTPUT
          elif [ -f "links.json" ]; then
            echo "üìÑ Found links.json file with public URLs"
            LINKS_CONTENT=$(cat links.json 2>/dev/null || echo "{}")
            echo "public_report_links=${LINKS_CONTENT}" >> $GITHUB_OUTPUT  
            echo "has_public_urls=true" >> $GITHUB_OUTPUT
          else
            echo "‚ö†Ô∏è No public URLs found - using local artifacts only"
            echo "has_public_urls=false" >> $GITHUB_OUTPUT
          fi
          
          # EXTRACT ASSERTION RESULTS FOR BUDGET VIOLATIONS
          echo "üìä CHECKING FOR ASSERTION RESULTS:"
          if [ -n "${{ steps.lighthouse-run.outputs.assertionResults }}" ]; then
            echo "‚úÖ Found assertion results: ${{ steps.lighthouse-run.outputs.assertionResults }}"
            echo "assertion_results=${{ steps.lighthouse-run.outputs.assertionResults }}" >> $GITHUB_OUTPUT
            echo "has_assertion_results=true" >> $GITHUB_OUTPUT
          else
            echo "‚ö†Ô∏è No assertion results found"
            echo "has_assertion_results=false" >> $GITHUB_OUTPUT
          fi
          
          # EXTRACT MANIFEST FOR DETAILED RUN INFORMATION
          echo "üìã CHECKING FOR LIGHTHOUSE MANIFEST:"
          if [ -n "${{ steps.lighthouse-run.outputs.manifest }}" ]; then
            echo "‚úÖ Found Lighthouse manifest with run details"
            echo "lighthouse_manifest=${{ steps.lighthouse-run.outputs.manifest }}" >> $GITHUB_OUTPUT
            echo "has_manifest=true" >> $GITHUB_OUTPUT
          else
            echo "‚ö†Ô∏è No manifest found"
            echo "has_manifest=false" >> $GITHUB_OUTPUT
          fi
          
          # Extract metrics even if Lighthouse CI fails due to budget violations
          if [ "${{ steps.lighthouse-run.outcome }}" = "success" ] || [ "${{ steps.lighthouse-run.outcome }}" = "failure" ]; then
            echo "=== LIGHTHOUSE CI DEBUG INFO ==="
            echo "Directory structure:"
            find .lighthouseci -type f 2>/dev/null | head -10 || echo "No .lighthouseci directory found"
            echo "JSON files found:"
            find .lighthouseci -name "*.json" -type f 2>/dev/null | head -5 || echo "No JSON files found"
            
            if [ -d ".lighthouseci" ] && [ -n "$(find .lighthouseci -name "*.json" -type f 2>/dev/null)" ]; then
              # Try multiple file patterns for Lighthouse CI JSON files
              LIGHTHOUSE_JSON=$(find .lighthouseci -name "lhr-*.json" -o -name "lighthouse-*.json" -o -name "*.report.json" -o -name "*.json" | head -1)
              
              echo "Selected JSON file: $LIGHTHOUSE_JSON"
              
              if [ -f "$LIGHTHOUSE_JSON" ] && [ -s "$LIGHTHOUSE_JSON" ]; then
                echo "Sample JSON structure:"
                head -10 "$LIGHTHOUSE_JSON" 2>/dev/null || echo "Could not read JSON file"
                
                # Enhanced metrics extraction with robust null handling and fallbacks
                PERFORMANCE_SCORE=$(jq -r '.categories.performance.score // null | if . != null then (. * 100 | floor) else "N/A" end' "$LIGHTHOUSE_JSON" 2>/dev/null || echo "N/A")
                ACCESSIBILITY_SCORE=$(jq -r '.categories.accessibility.score // null | if . != null then (. * 100 | floor) else "N/A" end' "$LIGHTHOUSE_JSON" 2>/dev/null || echo "N/A")
                BEST_PRACTICES_SCORE=$(jq -r '.categories["best-practices"].score // null | if . != null then (. * 100 | floor) else "N/A" end' "$LIGHTHOUSE_JSON" 2>/dev/null || echo "N/A")
                SEO_SCORE=$(jq -r '.categories.seo.score // null | if . != null then (. * 100 | floor) else "N/A" end' "$LIGHTHOUSE_JSON" 2>/dev/null || echo "N/A")
                
                # Core Web Vitals with enhanced parsing
                FCP=$(jq -r '.audits["first-contentful-paint"].displayValue // .audits["first-contentful-paint"].numericValue // null | if . != null then (if type == "number" then "\(.)ms" else . end) else "N/A" end' "$LIGHTHOUSE_JSON" 2>/dev/null || echo "N/A")
                LCP=$(jq -r '.audits["largest-contentful-paint"].displayValue // .audits["largest-contentful-paint"].numericValue // null | if . != null then (if type == "number" then "\(.)ms" else . end) else "N/A" end' "$LIGHTHOUSE_JSON" 2>/dev/null || echo "N/A")
                CLS=$(jq -r '.audits["cumulative-layout-shift"].displayValue // .audits["cumulative-layout-shift"].numericValue // null | if . != null then (if type == "number" then . else . end) else "N/A" end' "$LIGHTHOUSE_JSON" 2>/dev/null || echo "N/A")
                SPEED_INDEX=$(jq -r '.audits["speed-index"].displayValue // .audits["speed-index"].numericValue // null | if . != null then (if type == "number" then "\(.)ms" else . end) else "N/A" end' "$LIGHTHOUSE_JSON" 2>/dev/null || echo "N/A")
                
                # Debug extracted values
                echo "Extracted scores - Performance: $PERFORMANCE_SCORE, Accessibility: $ACCESSIBILITY_SCORE, Best Practices: $BEST_PRACTICES_SCORE, SEO: $SEO_SCORE"
                echo "Extracted vitals - FCP: $FCP, LCP: $LCP, CLS: $CLS, Speed Index: $SPEED_INDEX"
                
                # Export metrics to GitHub outputs
                echo "performance_score=${PERFORMANCE_SCORE}" >> $GITHUB_OUTPUT
                echo "accessibility_score=${ACCESSIBILITY_SCORE}" >> $GITHUB_OUTPUT  
                echo "best_practices_score=${BEST_PRACTICES_SCORE}" >> $GITHUB_OUTPUT
                echo "seo_score=${SEO_SCORE}" >> $GITHUB_OUTPUT
                echo "fcp=${FCP}" >> $GITHUB_OUTPUT
                echo "lcp=${LCP}" >> $GITHUB_OUTPUT
                echo "cls=${CLS}" >> $GITHUB_OUTPUT
                echo "speed_index=${SPEED_INDEX}" >> $GITHUB_OUTPUT
                echo "has_detailed_metrics=true" >> $GITHUB_OUTPUT
                
                echo "‚úÖ EXTRACTED METRICS FOR ${{ inputs.environment_name }}:"
                echo "Performance: ${PERFORMANCE_SCORE}%, Accessibility: ${ACCESSIBILITY_SCORE}%, Best Practices: ${BEST_PRACTICES_SCORE}%, SEO: ${SEO_SCORE}%"
                echo "Core Web Vitals: FCP=${FCP}, LCP=${LCP}, CLS=${CLS}, Speed Index=${SPEED_INDEX}"
              else
                echo "‚ùå Lighthouse JSON file not found or empty: $LIGHTHOUSE_JSON"
                echo "has_detailed_metrics=false" >> $GITHUB_OUTPUT
              fi
            else
              echo "‚ùå No .lighthouseci directory or JSON files found"
              echo "has_detailed_metrics=false" >> $GITHUB_OUTPUT
            fi
          else
            echo "‚ùå Lighthouse CI did not run successfully"
            echo "has_detailed_metrics=false" >> $GITHUB_OUTPUT
          fi

      - name: Upload Lighthouse Artifacts
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: lighthouse-${{ inputs.environment_name }}-${{ github.run_id }}
          path: |
            .lighthouseci/
            lighthouse-results.json
          retention-days: ${{ inputs.artifact_retention_days }}

  # Environment-Agnostic Accessibility Testing
  accessibility-testing:
    name: ‚ôø Accessibility (${{ inputs.environment_name }})
    runs-on: ubuntu-latest
    if: contains(inputs.test_types, 'accessibility')
    timeout-minutes: 5
    outputs:
      accessibility_summary: ${{ steps.process-axe-results.outputs.accessibility_summary }}
      violation_count: ${{ steps.process-axe-results.outputs.violation_count }}
      passes_count: ${{ steps.process-axe-results.outputs.passes_count }}
      incomplete_count: ${{ steps.process-axe-results.outputs.incomplete_count }}
      has_accessibility_results: ${{ steps.process-axe-results.outputs.has_accessibility_results }}
      axe_exit_code: ${{ steps.axe-scan.outputs.axe_exit_code }}
    steps:
      - name: Setup Node.js for Axe Core
        uses: actions/setup-node@v4
        with:
          node-version: '22'

      - name: Install Axe Core CLI for Accessibility Testing
        run: |
          echo "üì¶ Installing @axe-core/cli for comprehensive accessibility testing..."
          npm install -g @axe-core/cli
          axe --version

      - name: Run Axe Core Accessibility Scan
        id: axe-scan
        continue-on-error: true
        run: |
          echo "‚ôø Running axe-core accessibility scan on ${{ inputs.target_url }}"
          
          # Create output directory
          mkdir -p _accessibility-reports
          
          # Run axe-core scan with comprehensive output
          set +e  # Don't fail on accessibility violations
          
          axe "${{ inputs.target_url }}" \
            --save "axe-report.json" \
            --tags wcag2a,wcag2aa,wcag21aa,best-practice \
            --timeout 30000 \
            --verbose
          
          AXE_EXIT_CODE=$?
          echo "axe_exit_code=$AXE_EXIT_CODE" >> $GITHUB_OUTPUT
          
          # Copy the report to the expected location for artifact upload
          if [ -f "axe-report.json" ]; then
            cp "axe-report.json" "_accessibility-reports/axe-results.json"
          fi
          
          # Generate console output for PR comments and summaries
          axe --stdout "${{ inputs.target_url }}" \
            --tags wcag2a,wcag2aa,wcag21aa,best-practice \
            --timeout 30000 > "_accessibility-reports/axe-summary.txt" || true
          
          echo "‚úÖ Axe-core accessibility scan completed (exit code: $AXE_EXIT_CODE)"
          echo "üìÅ Reports generated in _accessibility-reports/"
          ls -la _accessibility-reports/ || true

      - name: Process Axe Core Results
        id: process-axe-results
        if: always()
        run: |
          echo "üîç Processing axe-core accessibility results..."
          
          # Install jq for JSON parsing
          if ! command -v jq >/dev/null 2>&1; then
            echo "üì¶ Installing jq for JSON parsing..."
            sudo apt-get update && sudo apt-get install -y jq
          fi
          
          # Check for axe-report.json first (primary file), then fallback to _accessibility-reports/axe-results.json
          RESULTS_FILE=""
          if [ -f "axe-report.json" ]; then
            RESULTS_FILE="axe-report.json"
          elif [ -f "_accessibility-reports/axe-results.json" ]; then
            RESULTS_FILE="_accessibility-reports/axe-results.json"
          fi
          
          if [ -n "$RESULTS_FILE" ]; then
            echo "üìÅ Found accessibility results in: $RESULTS_FILE"
            
            # Extract key metrics from axe-core JSON results
            VIOLATION_COUNT=$(jq -r '.violations | length' "$RESULTS_FILE" 2>/dev/null || echo "0")
            PASSES_COUNT=$(jq -r '.passes | length' "$RESULTS_FILE" 2>/dev/null || echo "0") 
            INCOMPLETE_COUNT=$(jq -r '.incomplete | length' "$RESULTS_FILE" 2>/dev/null || echo "0")
            
            echo "üìä Accessibility Results:"
            echo "- Violations: $VIOLATION_COUNT"
            echo "- Passes: $PASSES_COUNT" 
            echo "- Incomplete: $INCOMPLETE_COUNT"
            
            # Generate detailed violation summary
            if [ "$VIOLATION_COUNT" -gt "0" ]; then
              echo "üîç Detailed Violations:" 
              jq -r '.violations[] | "- \(.impact // "unknown") impact: \(.help) (\(.nodes | length) element(s))"' "$RESULTS_FILE" 2>/dev/null || echo "Could not parse violations"
              
              # Generate detailed violation breakdown for PR comments
              VIOLATIONS_DETAIL=$(jq -r '.violations[] | "**\(.id)**: \(.nodes | length) occurrences - \(.help)"' "$RESULTS_FILE" 2>/dev/null | head -10)
              
              ACCESSIBILITY_SUMMARY="‚ùå Found $VIOLATION_COUNT accessibility violations, $PASSES_COUNT checks passed, $INCOMPLETE_COUNT incomplete"
              echo "violations_detail<<EOF" >> $GITHUB_OUTPUT
              echo "$VIOLATIONS_DETAIL" >> $GITHUB_OUTPUT
              echo "EOF" >> $GITHUB_OUTPUT
            else
              ACCESSIBILITY_SUMMARY="‚úÖ No accessibility violations found! $PASSES_COUNT checks passed, $INCOMPLETE_COUNT incomplete"
              echo "violations_detail=" >> $GITHUB_OUTPUT
            fi
            
            # Export results for other steps
            echo "violation_count=$VIOLATION_COUNT" >> $GITHUB_OUTPUT
            echo "passes_count=$PASSES_COUNT" >> $GITHUB_OUTPUT
            echo "incomplete_count=$INCOMPLETE_COUNT" >> $GITHUB_OUTPUT
            echo "accessibility_summary=$ACCESSIBILITY_SUMMARY" >> $GITHUB_OUTPUT
            echo "has_accessibility_results=true" >> $GITHUB_OUTPUT
          else
            echo "‚ùå Axe results file not found"
            echo "accessibility_summary=‚ùå Accessibility scan failed - no results generated" >> $GITHUB_OUTPUT
            echo "has_accessibility_results=false" >> $GITHUB_OUTPUT
          fi
      - name: Upload Accessibility Artifacts
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: accessibility-${{ inputs.environment_name }}-${{ github.run_id }}
          path: |
            axe-report.json
            axe-report.html
            _accessibility-reports/
          retention-days: ${{ inputs.artifact_retention_days }}

  # Environment-Agnostic Performance Testing
  performance-testing:
    name: üìä Performance (${{ inputs.environment_name }})
    runs-on: ubuntu-latest
    if: contains(inputs.test_types, 'performance')
    timeout-minutes: 8
    outputs:
      performance_status: ${{ steps.vitals-test.outputs.performance_status }}
    steps:
      - name: Setup Node.js and Playwright
        uses: actions/setup-node@v4
        with:
          node-version: '22'

      - name: Install Playwright
        run: |
          npm install playwright
          npx playwright install chromium

      - name: Environment-Agnostic Performance Testing
        id: vitals-test
        run: |
          echo "üìä PERFORMANCE TESTING - UNIVERSAL URL APPROACH"
          echo "============================================"
          echo "üéØ TARGET URL: ${{ inputs.target_url }}"
          echo "üåç ENVIRONMENT: ${{ inputs.environment_name }}"
          echo "üìä CONTEXT: ${{ inputs.environment_context }}"
          echo "‚úÖ METHODOLOGY: Core Web Vitals measurement"
          echo ""
          
          # Create Core Web Vitals measurement script
          cat > measure-vitals.js << 'EOF'
          const { chromium } = require('playwright');
          
          (async () => {
            const browser = await chromium.launch();
            const page = await browser.newPage();
            
            try {
              console.log('üåê Navigating to URL...');
              await page.goto('${{ inputs.target_url }}', { waitUntil: 'networkidle', timeout: 30000 });
              
              const vitals = await page.evaluate(() => {
                return new Promise((resolve) => {
                  const metrics = {};
                  let metricsCollected = 0;
                  const expectedMetrics = 3;
                  
                  new PerformanceObserver((list) => {
                    const entries = list.getEntries();
                    entries.forEach((entry) => {
                      if (entry.name === 'FCP') {
                        metrics.fcp = entry.value;
                        metricsCollected++;
                      }
                      if (entry.name === 'LCP') {
                        metrics.lcp = entry.value;
                        metricsCollected++;
                      }
                      if (entry.name === 'CLS') {
                        metrics.cls = entry.value;
                        metricsCollected++;
                      }
                      
                      if (metricsCollected >= expectedMetrics) {
                        resolve(metrics);
                      }
                    });
                  }).observe({ entryTypes: ['largest-contentful-paint', 'first-contentful-paint', 'layout-shift'] });
                  
                  // Fallback timeout
                  setTimeout(() => resolve(metrics), 5000);
                });
              });
              
              console.log('üìä Core Web Vitals Results:');
              console.log(JSON.stringify(vitals, null, 2));
              
              // Save results
              require('fs').writeFileSync('core-web-vitals.json', JSON.stringify(vitals, null, 2));
              
            } catch (error) {
              console.error('‚ùå Performance testing failed:', error.message);
              process.exit(1);
            } finally {
              await browser.close();
            }
          })();
          EOF
          
          if node measure-vitals.js; then
            echo "‚úÖ Core Web Vitals measurement completed on ${{ inputs.environment_name }}"
            echo "performance_status=success" >> $GITHUB_OUTPUT
          else
            echo "‚ùå Performance measurement failed on ${{ inputs.environment_name }}"
            echo "performance_status=failed" >> $GITHUB_OUTPUT
            exit 1
          fi

      - name: Upload Performance Artifacts
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: performance-${{ inputs.environment_name }}-${{ github.run_id }}
          path: |
            core-web-vitals.json
            measure-vitals.js
          retention-days: ${{ inputs.artifact_retention_days }}

  # Comprehensive Results Summary
  test-results-summary:
    name: üìà Test Summary (${{ inputs.environment_name }})
    runs-on: ubuntu-latest
    needs: [lighthouse-testing, accessibility-testing, performance-testing]
    if: always()
    steps:
      - name: Generate Comprehensive Test Summary
        run: |
          echo "## üìà Comprehensive Testing Summary - ${{ inputs.environment_name }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**üéØ Target URL**: [${{ inputs.target_url }}](${{ inputs.target_url }})" >> $GITHUB_STEP_SUMMARY
          echo "**üåç Environment**: ${{ inputs.environment_name }}" >> $GITHUB_STEP_SUMMARY
          echo "**üìä Context**: ${{ inputs.environment_context }}" >> $GITHUB_STEP_SUMMARY
          echo "**üìÖ Tested**: $(date -u '+%Y-%m-%d %H:%M:%S UTC')" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Test Type | Status | Details |" >> $GITHUB_STEP_SUMMARY
          echo "|-----------|--------|---------|" >> $GITHUB_STEP_SUMMARY
          
          # Lighthouse Results
          if [ "${{ needs.lighthouse-testing.result }}" = "success" ]; then
            if [ "${{ needs.lighthouse-testing.outputs.has_detailed_metrics }}" = "true" ]; then
              echo "| üöÄ Lighthouse | ‚úÖ Success | Performance: ${{ needs.lighthouse-testing.outputs.performance_score }}%, Accessibility: ${{ needs.lighthouse-testing.outputs.accessibility_score }}% |" >> $GITHUB_STEP_SUMMARY
            else
              echo "| üöÄ Lighthouse | ‚úÖ Success | Metrics extraction failed |" >> $GITHUB_STEP_SUMMARY
            fi
          elif [ "${{ needs.lighthouse-testing.result }}" = "skipped" ]; then
            echo "| üöÄ Lighthouse | ‚è≠Ô∏è Skipped | Not requested |" >> $GITHUB_STEP_SUMMARY
          else
            echo "| üöÄ Lighthouse | ‚ùå Failed | Check logs for details |" >> $GITHUB_STEP_SUMMARY
          fi
          
          # Accessibility Results with detailed violation breakdown
          if [ "${{ needs.accessibility-testing.result }}" = "success" ]; then
            echo "| ‚ôø Accessibility | ‚úÖ Success | ${{ needs.accessibility-testing.outputs.accessibility_summary }} |" >> $GITHUB_STEP_SUMMARY
          elif [ "${{ needs.accessibility-testing.result }}" = "skipped" ]; then
            echo "| ‚ôø Accessibility | ‚è≠Ô∏è Skipped | Not requested |" >> $GITHUB_STEP_SUMMARY
          else
            echo "| ‚ôø Accessibility | ‚ùå Failed | ${{ needs.accessibility-testing.outputs.accessibility_summary }} |" >> $GITHUB_STEP_SUMMARY
            
            # Add detailed accessibility violation breakdown to summary
            if [ "${{ needs.accessibility-testing.outputs.violation_count }}" != "0" ] && [ -n "${{ needs.accessibility-testing.outputs.violation_count }}" ]; then
              echo "" >> $GITHUB_STEP_SUMMARY
              echo "### üîç Accessibility Violations Detected on ${{ inputs.environment_name }}" >> $GITHUB_STEP_SUMMARY
              echo "" >> $GITHUB_STEP_SUMMARY
              echo "**Violations**: ${{ needs.accessibility-testing.outputs.violation_count }}" >> $GITHUB_STEP_SUMMARY
              echo "**Passes**: ${{ needs.accessibility-testing.outputs.passes_count }}" >> $GITHUB_STEP_SUMMARY
              echo "**Incomplete**: ${{ needs.accessibility-testing.outputs.incomplete_count }}" >> $GITHUB_STEP_SUMMARY
              echo "" >> $GITHUB_STEP_SUMMARY
              echo "**üí° Action Required**: These violations prevent users with disabilities from accessing the site effectively." >> $GITHUB_STEP_SUMMARY
              echo "**üîß Next Steps**: Review detailed HTML and JSON reports in workflow artifacts for specific violation details." >> $GITHUB_STEP_SUMMARY
            fi
          fi
          
          # Performance Results
          if [ "${{ needs.performance-testing.result }}" = "success" ]; then
            echo "| üìä Performance | ‚úÖ Success | Core Web Vitals measured |" >> $GITHUB_STEP_SUMMARY
          elif [ "${{ needs.performance-testing.result }}" = "skipped" ]; then
            echo "| üìä Performance | ‚è≠Ô∏è Skipped | Not requested |" >> $GITHUB_STEP_SUMMARY
          else
            echo "| üìä Performance | ‚ùå Failed | Performance measurement failed |" >> $GITHUB_STEP_SUMMARY
          fi
          
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "üîó **Alternative Testing**: [PageSpeed Insights](${{ format('https://pagespeed.web.dev/analysis/{0}', inputs.target_url) }})" >> $GITHUB_STEP_SUMMARY

      - name: Post PR Comment (if requested)
        if: inputs.post_pr_comment == true && inputs.pr_number != ''
        uses: actions/github-script@v7
        with:
          script: |
            const prNumber = "${{ inputs.pr_number }}";
            const targetUrl = "${{ inputs.target_url }}";
            const environmentName = "${{ inputs.environment_name }}";
            const environmentContext = "${{ inputs.environment_context }}";
            
            let commentBody = `## üß™ Comprehensive Testing Results - ${environmentName}\n\n`;
            commentBody += `**üéØ Target URL**: [${targetUrl}](${targetUrl})\n`;
            commentBody += `**üåç Environment**: ${environmentName}\n`;
            commentBody += `**üìä Context**: ${environmentContext}\n`;
            commentBody += `**üìÖ Tested**: ${new Date().toISOString().split('T')[0]} ${new Date().toTimeString().split(' ')[0]} UTC\n\n`;
            
            // Add Lighthouse results if available
            const hasLighthouseMetrics = "${{ needs.lighthouse-testing.outputs.has_detailed_metrics }}" === "true";
            const hasAssertionResults = "${{ needs.lighthouse-testing.outputs.has_assertion_results }}" === "true";
            const hasPublicUrls = "${{ needs.lighthouse-testing.outputs.has_public_urls }}" === "true";
            
            if (hasLighthouseMetrics) {
              const performanceScore = "${{ needs.lighthouse-testing.outputs.performance_score }}";
              const accessibilityScore = "${{ needs.lighthouse-testing.outputs.accessibility_score }}";
              const bestPracticesScore = "${{ needs.lighthouse-testing.outputs.best_practices_score }}";
              const seoScore = "${{ needs.lighthouse-testing.outputs.seo_score }}";
              
              commentBody += `### üöÄ Lighthouse Scores\n\n`;
              commentBody += `| Category | Score | Status |\n`;
              commentBody += `|----------|-------|--------|\n`;
              commentBody += `| **Performance** | ${performanceScore}% | ${parseFloat(performanceScore) >= 85 ? '‚úÖ Good' : '‚ö†Ô∏è Needs Improvement'} |\n`;
              commentBody += `| **Accessibility** | ${accessibilityScore}% | ${parseFloat(accessibilityScore) >= 90 ? '‚úÖ Good' : '‚ö†Ô∏è Needs Improvement'} |\n`;
              commentBody += `| **Best Practices** | ${bestPracticesScore}% | ${parseFloat(bestPracticesScore) >= 85 ? '‚úÖ Good' : '‚ö†Ô∏è Needs Improvement'} |\n`;
              commentBody += `| **SEO** | ${seoScore}% | ${parseFloat(seoScore) >= 85 ? '‚úÖ Good' : '‚ö†Ô∏è Needs Improvement'} |\n\n`;
              
              // Add Core Web Vitals if available
              const fcp = "${{ needs.lighthouse-testing.outputs.fcp }}";
              const lcp = "${{ needs.lighthouse-testing.outputs.lcp }}";
              const cls = "${{ needs.lighthouse-testing.outputs.cls }}";
              const speedIndex = "${{ needs.lighthouse-testing.outputs.speed_index }}";
              
              if (fcp !== "N/A" || lcp !== "N/A" || cls !== "N/A") {
                commentBody += `### ‚ö° Core Web Vitals\n\n`;
                commentBody += `| Metric | Value | Status |\n`;
                commentBody += `|--------|-------|--------|\n`;
                if (fcp !== "N/A") commentBody += `| **First Contentful Paint** | ${fcp} | ${fcp.includes('s') && parseFloat(fcp) <= 1.8 ? '‚úÖ Good' : '‚ö†Ô∏è Needs Improvement'} |\n`;
                if (lcp !== "N/A") commentBody += `| **Largest Contentful Paint** | ${lcp} | ${lcp.includes('s') && parseFloat(lcp) <= 2.5 ? '‚úÖ Good' : '‚ö†Ô∏è Needs Improvement'} |\n`;
                if (cls !== "N/A") commentBody += `| **Cumulative Layout Shift** | ${cls} | ${parseFloat(cls) <= 0.1 ? '‚úÖ Good' : '‚ö†Ô∏è Needs Improvement'} |\n`;
                if (speedIndex !== "N/A") commentBody += `| **Speed Index** | ${speedIndex} | ${speedIndex.includes('s') && parseFloat(speedIndex) <= 3.4 ? '‚úÖ Good' : '‚ö†Ô∏è Needs Improvement'} |\n`;
                commentBody += `\n`;
              }
            }
            
            // Add assertion results for budget violations
            if (hasAssertionResults) {
              try {
                const assertionResults = JSON.parse('${{ needs.lighthouse-testing.outputs.assertion_results }}');
                if (assertionResults && assertionResults.length > 0) {
                  commentBody += `### üìä Performance Budget Results\n\n`;
                  commentBody += `| Assertion | Status | Details |\n`;
                  commentBody += `|-----------|--------|---------|\n`;
                  
                  assertionResults.forEach(result => {
                    const status = result.level === 'error' ? '‚ùå Failed' : result.level === 'warn' ? '‚ö†Ô∏è Warning' : '‚úÖ Passed';
                    const auditName = result.auditId || result.name || 'Unknown';
                    const details = result.actual ? `Expected: ${result.expected || 'N/A'}, Actual: ${result.actual}` : 'Budget check';
                    commentBody += `| **${auditName}** | ${status} | ${details} |\n`;
                  });
                  commentBody += `\n`;
                }
              } catch (e) {
                console.log('Could not parse assertion results:', e.message);
              }
            }
            
            // Add public report links if available
            if (hasPublicUrls) {
              try {
                const publicLinks = JSON.parse('${{ needs.lighthouse-testing.outputs.public_report_links }}');
                if (publicLinks && publicLinks.length > 0) {
                  commentBody += `### üîó Public Lighthouse Reports\n\n`;
                  publicLinks.forEach((link, index) => {
                    commentBody += `üìä [**Detailed Report ${index + 1}**](${link}) - Complete Lighthouse analysis\n`;
                  });
                  commentBody += `\n`;
                }
              } catch (e) {
                console.log('Could not parse public report links:', e.message);
              }
            }
            
            commentBody += `### üìä Test Summary\n\n`;
            commentBody += `| Test Type | Status | Details |\n`;
            commentBody += `|-----------|--------|---------|\n`;
            
            // Add detailed accessibility section using axe-core results
            const violationCount = "${{ needs.accessibility-testing.outputs.violation_count }}";
            const passesCount = "${{ needs.accessibility-testing.outputs.passes_count }}";
            const incompleteCount = "${{ needs.accessibility-testing.outputs.incomplete_count }}";
            const hasAccessibilityResults = "${{ needs.accessibility-testing.outputs.has_accessibility_results }}";
            const accessibilitySummary = "${{ needs.accessibility-testing.outputs.accessibility_summary }}";
            
            if (hasAccessibilityResults === 'true' && violationCount !== "0" && violationCount !== "") {
              commentBody += `### ‚ôø Accessibility Issues Detected\n\n`;
              commentBody += `**üö® ${violationCount} WCAG 2.1 AA violations found using axe-core!**\n\n`;
              commentBody += `| Result Type | Count | Status |\n`;
              commentBody += `|-------------|-------|--------|\n`;
              commentBody += `| ‚ùå Violations | ${violationCount} | Need fixing |\n`;
              commentBody += `| ‚úÖ Passes | ${passesCount} | Working correctly |\n`;
              commentBody += `| ‚ö†Ô∏è Incomplete | ${incompleteCount} | Manual review needed |\n\n`;
              
              commentBody += `**üí° Action Required**: These violations prevent users with disabilities from accessing the site effectively.\n`;
              commentBody += `**üîß Next Steps**: Review detailed HTML report in workflow artifacts for specific violation details.\n\n`;
            } else if (hasAccessibilityResults === 'true' && violationCount === "0") {
              commentBody += `### ‚ôø Accessibility Status: EXCELLENT! üéâ\n\n`;
              commentBody += `**‚úÖ No accessibility violations found!**\n\n`;
              commentBody += `| Result Type | Count | Status |\n`;
              commentBody += `|-------------|-------|--------|\n`;
              commentBody += `| ‚úÖ Passes | ${passesCount} | WCAG 2.1 AA compliant |\n`;
              commentBody += `| ‚ö†Ô∏è Incomplete | ${incompleteCount} | Manual review recommended |\n\n`;
            }
            
            // Add test results using proper JavaScript variables
            const lighthouseStatus = "${{ needs.lighthouse-testing.result }}" === 'success' ? '‚úÖ Success' : 
                                   "${{ needs.lighthouse-testing.result }}" === 'skipped' ? '‚è≠Ô∏è Skipped' : '‚ùå Failed';
            const accessibilityStatus = "${{ needs.accessibility-testing.result }}" === 'success' ? '‚úÖ Success' : 
                                       "${{ needs.accessibility-testing.result }}" === 'skipped' ? '‚è≠Ô∏è Skipped' : '‚ùå Failed';
            const performanceStatus = "${{ needs.performance-testing.result }}" === 'success' ? '‚úÖ Success' : 
                                     "${{ needs.performance-testing.result }}" === 'skipped' ? '‚è≠Ô∏è Skipped' : '‚ùå Failed';
            
            commentBody += `| üöÄ Lighthouse | ${lighthouseStatus} | Performance and quality analysis |\n`;
            commentBody += `| ‚ôø Accessibility | ${accessibilityStatus} | ${{ needs.accessibility-testing.outputs.accessibility_summary }} |\n`;
            commentBody += `| üìä Performance | ${performanceStatus} | Core Web Vitals measurement |\n\n`;
            
            commentBody += `üîó **Alternative Testing**: [PageSpeed Insights](https://pagespeed.web.dev/analysis/${encodeURIComponent(targetUrl)})\n\n`;
            commentBody += `---\n*ü§ñ Multi-environment testing via reusable workflow - ${environmentName} environment*\n\n`;
            commentBody += `<!-- comprehensive-testing-${environmentName}-${Date.now()} -->`;
            
            // Post comment
            await github.rest.issues.createComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: prNumber,
              body: commentBody,
            });
            
            console.log(`‚úÖ Posted comprehensive testing results to PR #${prNumber} for ${environmentName} environment`);
---
name: 🧪 Shared Comprehensive Testing Suite

on:
  workflow_call:
    inputs:
      target_url:
        description: 'URL to test (any environment)'
        required: true
        type: string
      environment_name:
        description: 'Environment name (preview, staging, production)'
        required: true
        type: string
      environment_context:
        description: 'Additional context (PR number, release tag, etc.)'
        required: false
        type: string
        default: ''
      test_types:
        description: 'Comma-separated test types to run (lighthouse,accessibility,security,performance)'
        required: false
        type: string
        default: 'lighthouse,accessibility,performance'
      lighthouse_budget_path:
        description: 'Path to Lighthouse budget.json file'
        required: false
        type: string
        default: './budget.json'
      artifact_retention_days:
        description: 'How many days to retain test artifacts'
        required: false
        type: number
        default: 7
      post_pr_comment:
        description: 'Whether to post results to PR comment (only works for PR context)'
        required: false
        type: boolean
        default: false
      pr_number:
        description: 'PR number for comment posting (required if post_pr_comment is true)'
        required: false
        type: string
        default: ''

permissions:
  contents: read
  pull-requests: write
  checks: write

jobs:
  # Environment-Agnostic Lighthouse Testing
  lighthouse-testing:
    name: 🚀 Lighthouse (${{ inputs.environment_name }})
    runs-on: ubuntu-latest
    if: contains(inputs.test_types, 'lighthouse')
    timeout-minutes: 10
    outputs:
      performance_score: ${{ steps.lighthouse-results.outputs.performance_score }}
      accessibility_score: ${{ steps.lighthouse-results.outputs.accessibility_score }}
      best_practices_score: ${{ steps.lighthouse-results.outputs.best_practices_score }}
      seo_score: ${{ steps.lighthouse-results.outputs.seo_score }}
      fcp: ${{ steps.lighthouse-results.outputs.fcp }}
      lcp: ${{ steps.lighthouse-results.outputs.lcp }}
      cls: ${{ steps.lighthouse-results.outputs.cls }}
      speed_index: ${{ steps.lighthouse-results.outputs.speed_index }}
      has_detailed_metrics: ${{ steps.lighthouse-results.outputs.has_detailed_metrics }}
      public_report_links: ${{ steps.lighthouse-results.outputs.public_report_links }}
      has_public_urls: ${{ steps.lighthouse-results.outputs.has_public_urls }}
      assertion_results: ${{ steps.lighthouse-results.outputs.assertion_results }}
      has_assertion_results: ${{ steps.lighthouse-results.outputs.has_assertion_results }}
      lighthouse_manifest: ${{ steps.lighthouse-results.outputs.lighthouse_manifest }}
      has_manifest: ${{ steps.lighthouse-results.outputs.has_manifest }}
    steps:
      - name: Checkout for budget configuration
        uses: actions/checkout@v5
        with:
          sparse-checkout: |
            budget.json
            
      - name: Environment-Agnostic Lighthouse Testing
        run: |
          echo "🚀 LIGHTHOUSE TESTING - UNIVERSAL URL APPROACH"
          echo "=============================================="
          echo "🎯 TARGET URL: ${{ inputs.target_url }}"
          echo "🌍 ENVIRONMENT: ${{ inputs.environment_name }}"
          echo "📊 CONTEXT: ${{ inputs.environment_context }}"
          echo "✅ METHODOLOGY: Google Lighthouse CI with performance budgets"
          echo "🔗 ADVANTAGE: Tests real user experience on any deployment"
          echo ""
          
          # Create dynamic Lighthouse config for ANY environment
          cat > .lighthouserc.json << 'EOF'
          {
            "ci": {
              "collect": {
                "url": ["${{ inputs.target_url }}"],
                "numberOfRuns": 3,
                "settings": {
                  "preset": "desktop",
                  "onlyCategories": ["performance", "accessibility", "best-practices", "seo"],
                  "chromeFlags": "--no-sandbox --headless=new --disable-gpu --disable-dev-shm-usage --disable-extensions --no-first-run --disable-default-apps --disable-background-timer-throttling --disable-backgrounding-occluded-windows --disable-renderer-backgrounding",
                  "maxWaitForLoad": 60000,
                  "maxWaitForFcp": 20000,
                  "skipAudits": ["uses-http2"],
                  "throttling": {
                    "rttMs": 40,
                    "throughputKbps": 10240,
                    "cpuSlowdownMultiplier": 1,
                    "requestLatencyMs": 0,
                    "downloadThroughputKbps": 0,
                    "uploadThroughputKbps": 0
                  }
                }
              },
              "assert": {
                "budgetPath": "${{ inputs.lighthouse_budget_path }}",
                "assertions": {
                  "categories:performance": ["warn", {"minScore": 0.85}],
                  "categories:accessibility": ["warn", {"minScore": 0.90}],
                  "categories:best-practices": ["warn", {"minScore": 0.85}],
                  "categories:seo": ["warn", {"minScore": 0.85}],
                  "categories:pwa": "off",
                  "first-contentful-paint": ["warn", {"maxNumericValue": 2000}],
                  "largest-contentful-paint": ["warn", {"maxNumericValue": 2500}],
                  "cumulative-layout-shift": ["warn", {"maxNumericValue": 0.1}],
                  "speed-index": ["warn", {"maxNumericValue": 3000}]
                }
              },
              "upload": {
                "target": "temporary-public-storage",
                "temporaryPublicStorage": true
              }
            }
          }
          EOF

      - name: Execute Lighthouse CI
        uses: treosh/lighthouse-ci-action@v12
        with:
          configPath: '.lighthouserc.json'
          uploadArtifacts: true
        continue-on-error: true
        id: lighthouse-run

      - name: Extract Lighthouse Metrics
        id: lighthouse-results
        run: |
          echo "📊 EXTRACTING LIGHTHOUSE METRICS FOR ${{ inputs.environment_name }}"
          
          # Install jq for JSON parsing
          if ! command -v jq >/dev/null 2>&1; then
            sudo apt-get update && sudo apt-get install -y jq
          fi
          
          # EXTRACT TEMPORARY PUBLIC STORAGE URLS AND ASSERTION RESULTS
          echo "🔗 CHECKING FOR TEMPORARY PUBLIC STORAGE URLS:"
          if [ -n "${{ steps.lighthouse-run.outputs.links }}" ]; then
            echo "✅ Found public report links: ${{ steps.lighthouse-run.outputs.links }}"
            echo "public_report_links=${{ steps.lighthouse-run.outputs.links }}" >> $GITHUB_OUTPUT
            echo "has_public_urls=true" >> $GITHUB_OUTPUT
          elif [ -f "links.json" ]; then
            echo "📄 Found links.json file with public URLs"
            LINKS_CONTENT=$(cat links.json 2>/dev/null || echo "{}")
            echo "public_report_links=${LINKS_CONTENT}" >> $GITHUB_OUTPUT  
            echo "has_public_urls=true" >> $GITHUB_OUTPUT
          else
            echo "⚠️ No public URLs found - using local artifacts only"
            echo "has_public_urls=false" >> $GITHUB_OUTPUT
          fi
          
          # EXTRACT ASSERTION RESULTS FOR BUDGET VIOLATIONS
          echo "📊 CHECKING FOR ASSERTION RESULTS:"
          if [ -n "${{ steps.lighthouse-run.outputs.assertionResults }}" ]; then
            echo "✅ Found assertion results: ${{ steps.lighthouse-run.outputs.assertionResults }}"
            echo "assertion_results=${{ steps.lighthouse-run.outputs.assertionResults }}" >> $GITHUB_OUTPUT
            echo "has_assertion_results=true" >> $GITHUB_OUTPUT
          else
            echo "⚠️ No assertion results found"
            echo "has_assertion_results=false" >> $GITHUB_OUTPUT
          fi
          
          # EXTRACT MANIFEST FOR DETAILED RUN INFORMATION
          echo "📋 CHECKING FOR LIGHTHOUSE MANIFEST:"
          if [ -n "${{ steps.lighthouse-run.outputs.manifest }}" ]; then
            echo "✅ Found Lighthouse manifest with run details"
            echo "lighthouse_manifest=${{ steps.lighthouse-run.outputs.manifest }}" >> $GITHUB_OUTPUT
            echo "has_manifest=true" >> $GITHUB_OUTPUT
          else
            echo "⚠️ No manifest found"
            echo "has_manifest=false" >> $GITHUB_OUTPUT
          fi
          
          # Extract metrics even if Lighthouse CI fails due to budget violations
          if [ "${{ steps.lighthouse-run.outcome }}" = "success" ] || [ "${{ steps.lighthouse-run.outcome }}" = "failure" ]; then
            echo "=== LIGHTHOUSE CI DEBUG INFO ==="
            echo "Directory structure:"
            find .lighthouseci -type f 2>/dev/null | head -10 || echo "No .lighthouseci directory found"
            echo "JSON files found:"
            find .lighthouseci -name "*.json" -type f 2>/dev/null | head -5 || echo "No JSON files found"
            
            if [ -d ".lighthouseci" ] && [ -n "$(find .lighthouseci -name "*.json" -type f 2>/dev/null)" ]; then
              # Try multiple file patterns for Lighthouse CI JSON files
              LIGHTHOUSE_JSON=$(find .lighthouseci -name "lhr-*.json" -o -name "lighthouse-*.json" -o -name "*.report.json" -o -name "*.json" | head -1)
              
              echo "Selected JSON file: $LIGHTHOUSE_JSON"
              
              if [ -f "$LIGHTHOUSE_JSON" ] && [ -s "$LIGHTHOUSE_JSON" ]; then
                echo "Sample JSON structure:"
                head -10 "$LIGHTHOUSE_JSON" 2>/dev/null || echo "Could not read JSON file"
                
                # Enhanced metrics extraction with robust null handling and fallbacks
                PERFORMANCE_SCORE=$(jq -r '.categories.performance.score // null | if . != null then (. * 100 | floor) else "N/A" end' "$LIGHTHOUSE_JSON" 2>/dev/null || echo "N/A")
                ACCESSIBILITY_SCORE=$(jq -r '.categories.accessibility.score // null | if . != null then (. * 100 | floor) else "N/A" end' "$LIGHTHOUSE_JSON" 2>/dev/null || echo "N/A")
                BEST_PRACTICES_SCORE=$(jq -r '.categories["best-practices"].score // null | if . != null then (. * 100 | floor) else "N/A" end' "$LIGHTHOUSE_JSON" 2>/dev/null || echo "N/A")
                SEO_SCORE=$(jq -r '.categories.seo.score // null | if . != null then (. * 100 | floor) else "N/A" end' "$LIGHTHOUSE_JSON" 2>/dev/null || echo "N/A")
                
                # Core Web Vitals with enhanced parsing
                FCP=$(jq -r '.audits["first-contentful-paint"].displayValue // .audits["first-contentful-paint"].numericValue // null | if . != null then (if type == "number" then "\(.)ms" else . end) else "N/A" end' "$LIGHTHOUSE_JSON" 2>/dev/null || echo "N/A")
                LCP=$(jq -r '.audits["largest-contentful-paint"].displayValue // .audits["largest-contentful-paint"].numericValue // null | if . != null then (if type == "number" then "\(.)ms" else . end) else "N/A" end' "$LIGHTHOUSE_JSON" 2>/dev/null || echo "N/A")
                CLS=$(jq -r '.audits["cumulative-layout-shift"].displayValue // .audits["cumulative-layout-shift"].numericValue // null | if . != null then (if type == "number" then . else . end) else "N/A" end' "$LIGHTHOUSE_JSON" 2>/dev/null || echo "N/A")
                SPEED_INDEX=$(jq -r '.audits["speed-index"].displayValue // .audits["speed-index"].numericValue // null | if . != null then (if type == "number" then "\(.)ms" else . end) else "N/A" end' "$LIGHTHOUSE_JSON" 2>/dev/null || echo "N/A")
                
                # Debug extracted values
                echo "Extracted scores - Performance: $PERFORMANCE_SCORE, Accessibility: $ACCESSIBILITY_SCORE, Best Practices: $BEST_PRACTICES_SCORE, SEO: $SEO_SCORE"
                echo "Extracted vitals - FCP: $FCP, LCP: $LCP, CLS: $CLS, Speed Index: $SPEED_INDEX"
                
                # Export metrics to GitHub outputs
                echo "performance_score=${PERFORMANCE_SCORE}" >> $GITHUB_OUTPUT
                echo "accessibility_score=${ACCESSIBILITY_SCORE}" >> $GITHUB_OUTPUT  
                echo "best_practices_score=${BEST_PRACTICES_SCORE}" >> $GITHUB_OUTPUT
                echo "seo_score=${SEO_SCORE}" >> $GITHUB_OUTPUT
                echo "fcp=${FCP}" >> $GITHUB_OUTPUT
                echo "lcp=${LCP}" >> $GITHUB_OUTPUT
                echo "cls=${CLS}" >> $GITHUB_OUTPUT
                echo "speed_index=${SPEED_INDEX}" >> $GITHUB_OUTPUT
                echo "has_detailed_metrics=true" >> $GITHUB_OUTPUT
                
                echo "✅ EXTRACTED METRICS FOR ${{ inputs.environment_name }}:"
                echo "Performance: ${PERFORMANCE_SCORE}%, Accessibility: ${ACCESSIBILITY_SCORE}%, Best Practices: ${BEST_PRACTICES_SCORE}%, SEO: ${SEO_SCORE}%"
                echo "Core Web Vitals: FCP=${FCP}, LCP=${LCP}, CLS=${CLS}, Speed Index=${SPEED_INDEX}"
              else
                echo "❌ Lighthouse JSON file not found or empty: $LIGHTHOUSE_JSON"
                echo "has_detailed_metrics=false" >> $GITHUB_OUTPUT
              fi
            else
              echo "❌ No .lighthouseci directory or JSON files found"
              echo "has_detailed_metrics=false" >> $GITHUB_OUTPUT
            fi
          else
            echo "❌ Lighthouse CI did not run successfully"
            echo "has_detailed_metrics=false" >> $GITHUB_OUTPUT
          fi

      - name: Upload Lighthouse Artifacts
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: lighthouse-${{ inputs.environment_name }}-${{ github.run_id }}
          path: |
            .lighthouseci/
            lighthouse-results.json
          retention-days: ${{ inputs.artifact_retention_days }}

  # Environment-Agnostic Accessibility Testing
  accessibility-testing:
    name: ♿ Accessibility (${{ inputs.environment_name }})
    runs-on: ubuntu-latest
    if: contains(inputs.test_types, 'accessibility')
    timeout-minutes: 10
    outputs:
      # Core accessibility metrics (generic/dynamic)
      accessibility_summary: ${{ steps.process-axe-results.outputs.accessibility_summary }}
      violation_count: ${{ steps.process-axe-results.outputs.violation_count }}
      passes_count: ${{ steps.process-axe-results.outputs.passes_count }}
      incomplete_count: ${{ steps.process-axe-results.outputs.incomplete_count }}
      has_accessibility_results: ${{ steps.process-axe-results.outputs.has_accessibility_results }}
      violations_json: ${{ steps.process-axe-results.outputs.violations_json }}
      axe_exit_code: ${{ steps.axe-scan.outputs.axe_exit_code }}
      
      # Enhanced WCAG 2.2 compliance outputs (marketplace-ready)
      wcag22a_violations: ${{ steps.process-axe-results.outputs.wcag22a_violations }}
      wcag22aa_violations: ${{ steps.process-axe-results.outputs.wcag22aa_violations }}
      wcag22aaa_violations: ${{ steps.process-axe-results.outputs.wcag22aaa_violations }}
      best_practice_violations: ${{ steps.process-axe-results.outputs.best_practice_violations }}
      critical_rule_violations: ${{ steps.process-axe-results.outputs.critical_rule_violations }}
      
      # Rule category analysis (generic/extensible)
      color_violations: ${{ steps.process-axe-results.outputs.color_violations }}
      navigation_violations: ${{ steps.process-axe-results.outputs.navigation_violations }}
      form_violations: ${{ steps.process-axe-results.outputs.form_violations }}
      aria_violations: ${{ steps.process-axe-results.outputs.aria_violations }}
      
      # Automation categorization (AI/tooling friendly)
      automated_violations: ${{ steps.process-axe-results.outputs.automated_violations }}
      manual_review_violations: ${{ steps.process-axe-results.outputs.manual_review_violations }}
    steps:
      - name: Setup Node.js for Axe Core
        uses: actions/setup-node@v4
        with:
          node-version: '22'

      - name: Install Axe Core CLI for Accessibility Testing
        run: |
          echo "📦 Installing @axe-core/cli for comprehensive accessibility testing..."
          npm install -g @axe-core/cli
          axe --version

      - name: Run Axe Core Accessibility Scan
        id: axe-scan
        continue-on-error: true
        run: |
          echo "♿ Running axe-core accessibility scan on ${{ inputs.target_url }}"
          
          # Create output directory
          mkdir -p _accessibility-reports
          
          # Run axe-core scan with comprehensive output using same Chrome options as individual test
          set +e  # Don't fail on accessibility violations
          
          # Optimized: Single comprehensive scan with safe and documented WCAG tags
          echo "🚀 Optimized WCAG 2.1 AA Comprehensive Accessibility Scan..."
          axe "${{ inputs.target_url }}" \
            --save "axe-report.json" \
            --tags wcag2a,wcag2aa,wcag21aa,best-practice \
            --chrome-options="--no-sandbox,--disable-setuid-sandbox,--disable-dev-shm-usage,--disable-extensions,--disable-gpu,--headless=new" \
            --timeout 45000 \
            --verbose
          
          AXE_EXIT_CODE=$?
          echo "axe_exit_code=$AXE_EXIT_CODE" >> $GITHUB_OUTPUT
          
          # Copy the report to expected locations for processing
          if [ -f "axe-report.json" ]; then
            cp "axe-report.json" "_accessibility-reports/axe-results.json"
            # Create simplified versions for compatibility with existing processing
            cp "axe-report.json" "_accessibility-reports/axe-wcag22a.json"
            cp "axe-report.json" "_accessibility-reports/axe-wcag22aa.json"  
            cp "axe-report.json" "_accessibility-reports/axe-wcag22aaa.json"
            cp "axe-report.json" "_accessibility-reports/axe-best-practices.json"
            cp "axe-report.json" "_accessibility-reports/axe-critical-rules.json"
          fi
          
          echo "✅ Axe-core accessibility scan completed (exit code: $AXE_EXIT_CODE)"
          echo "📁 Reports generated in _accessibility-reports/"
          ls -la _accessibility-reports/ || true

      - name: Process Axe Core Results
        id: process-axe-results
        if: always()
        run: |
          echo "🔍 Processing axe-core accessibility results..."
          
          # Install jq for JSON parsing
          if ! command -v jq >/dev/null 2>&1; then
            echo "📦 Installing jq for JSON parsing..."
            sudo apt-get update && sudo apt-get install -y jq
          fi
          
          # Check for axe-report.json first (primary file), then fallback to _accessibility-reports/axe-results.json
          RESULTS_FILE=""
          if [ -f "axe-report.json" ]; then
            RESULTS_FILE="axe-report.json"
          elif [ -f "_accessibility-reports/axe-results.json" ]; then
            RESULTS_FILE="_accessibility-reports/axe-results.json"
          fi
          
          if [ -n "$RESULTS_FILE" ]; then
            echo "📁 Found accessibility results in: $RESULTS_FILE"
            echo "📊 File size: $(wc -c < "$RESULTS_FILE" 2>/dev/null || echo "unknown") bytes"
            echo "🔍 First 500 characters of JSON:"
            head -c 500 "$RESULTS_FILE" 2>/dev/null || echo "Could not read file"
            echo ""
            echo "🔍 JSON validation check:"
            if jq empty "$RESULTS_FILE" 2>/dev/null; then
              echo "✅ JSON is valid"
            else
              echo "❌ JSON is invalid or malformed"
              echo "📄 Raw file content:"
              cat "$RESULTS_FILE" || echo "Could not read file"
            fi
            echo ""
            
            # Extract key metrics from axe-core JSON results with enhanced debugging
            echo "🔍 Parsing JSON structure..."
            echo "Available top-level keys:"
            jq -r 'keys[]' "$RESULTS_FILE" 2>/dev/null || echo "Could not list keys"
            echo ""
            
            # Axe-core generates array format, so access first element [0]
            # Count total individual violations (sum of all nodes arrays)
            VIOLATION_COUNT=$(jq -r '.[0].violations | map(.nodes | length) | add // 0' "$RESULTS_FILE" 2>/dev/null || echo "0")
            VIOLATION_TYPES=$(jq -r '.[0].violations | length' "$RESULTS_FILE" 2>/dev/null || echo "0")
            PASSES_COUNT=$(jq -r '.[0].passes | length' "$RESULTS_FILE" 2>/dev/null || echo "0") 
            INCOMPLETE_COUNT=$(jq -r '.[0].incomplete | length' "$RESULTS_FILE" 2>/dev/null || echo "0")
            
            echo "📊 Raw jq parsing results (using .[0] for array format):"
            echo "- Violations raw: $(jq -r '.[0].violations' "$RESULTS_FILE" 2>/dev/null | head -c 100 || echo "failed")"
            echo "- Passes raw: $(jq -r '.[0].passes' "$RESULTS_FILE" 2>/dev/null | head -c 100 || echo "failed")"
            echo "- Incomplete raw: $(jq -r '.[0].incomplete' "$RESULTS_FILE" 2>/dev/null | head -c 100 || echo "failed")"
            echo ""
            echo "📊 Final parsed counts:"
            echo "- Total Individual Violations: $VIOLATION_COUNT (across $VIOLATION_TYPES violation types)"
            echo "- Passes: $PASSES_COUNT" 
            echo "- Incomplete: $INCOMPLETE_COUNT"
            
            # Debug: Show breakdown of violation types and their counts
            if [ "$VIOLATION_COUNT" -gt "0" ]; then
              echo "🔍 Violation breakdown by type:"
              jq -r '.[0].violations[] | "  - \(.id): \(.nodes | length) occurrences"' "$RESULTS_FILE" 2>/dev/null || echo "  Could not parse breakdown"
            fi
            
            # Generate detailed violation summary
            if [ "$VIOLATION_COUNT" -gt "0" ]; then
              echo "🔍 Detailed Violations:" 
              jq -r '.[0].violations[] | "- \(.impact // "unknown") impact: \(.help) (\(.nodes | length) element(s))"' "$RESULTS_FILE" 2>/dev/null || echo "Could not parse violations"
              
              # Generate enhanced violation breakdown for PR comments with URLs and details
              VIOLATIONS_DETAIL=$(jq -r '.[0].violations[] | "**\(.id)**: \(.nodes | length) occurrences\n- **Description**: \(.help)\n- **Impact**: \(.impact // "unknown")\n- **Help URL**: \(.helpUrl)\n- **WCAG Tags**: \(.tags | join(", "))\n"' "$RESULTS_FILE" 2>/dev/null)
              
              # Generate creative, organized violation sections by impact level
              VIOLATIONS_COLLAPSIBLE=""
              VIOLATIONS_PROCESSED=0
              
              # Initialize counters by impact level
              CRITICAL_COUNT=0
              SERIOUS_COUNT=0 
              MODERATE_COUNT=0
              MINOR_COUNT=0
              
              ACCESSIBILITY_SUMMARY="❌ Found $VIOLATION_COUNT individual accessibility violations across $VIOLATION_TYPES violation types, $PASSES_COUNT checks passed, $INCOMPLETE_COUNT incomplete"
              
              # Simply pass the violations JSON to JavaScript for processing - much simpler!
              VIOLATIONS_JSON=$(jq -c '.[0].violations' "$RESULTS_FILE" 2>/dev/null || echo "[]")
              echo "violations_json=$VIOLATIONS_JSON" >> $GITHUB_OUTPUT
              echo "✅ Violations JSON passed (${#VIOLATIONS_JSON} chars) - will be processed in JavaScript"
            else
              ACCESSIBILITY_SUMMARY="✅ No accessibility violations found! $PASSES_COUNT checks passed, $INCOMPLETE_COUNT incomplete"
              echo "violations_json=[]" >> $GITHUB_OUTPUT
            fi
            
            # Enhanced WCAG Level Analysis and Rule Category Processing
            echo "🎯 Processing enhanced WCAG 2.2 compliance analysis..."
            
            # Process WCAG Level-based results if files exist
            WCAG22A_VIOLATIONS=0
            WCAG22AA_VIOLATIONS=0
            WCAG22AAA_VIOLATIONS=0
            BEST_PRACTICE_VIOLATIONS=0
            CRITICAL_RULE_VIOLATIONS=0
            
            # WCAG 2.2 Level A Analysis
            if [ -f "_accessibility-reports/axe-wcag22a.json" ]; then
              WCAG22A_VIOLATIONS=$(jq -r '.[0].violations | map(.nodes | length) | add // 0' "_accessibility-reports/axe-wcag22a.json" 2>/dev/null || echo "0")
              echo "📋 WCAG 2.2 Level A: $WCAG22A_VIOLATIONS violations"
            fi
            
            # WCAG 2.2 Level AA Analysis
            if [ -f "_accessibility-reports/axe-wcag22aa.json" ]; then
              WCAG22AA_VIOLATIONS=$(jq -r '.[0].violations | map(.nodes | length) | add // 0' "_accessibility-reports/axe-wcag22aa.json" 2>/dev/null || echo "0")
              echo "📋 WCAG 2.2 Level AA: $WCAG22AA_VIOLATIONS violations"
            fi
            
            # WCAG 2.2 Level AAA Analysis
            if [ -f "_accessibility-reports/axe-wcag22aaa.json" ]; then
              WCAG22AAA_VIOLATIONS=$(jq -r '.[0].violations | map(.nodes | length) | add // 0' "_accessibility-reports/axe-wcag22aaa.json" 2>/dev/null || echo "0")
              echo "📋 WCAG 2.2 Level AAA: $WCAG22AAA_VIOLATIONS violations"
            fi
            
            # Best Practices Analysis
            if [ -f "_accessibility-reports/axe-best-practices.json" ]; then
              BEST_PRACTICE_VIOLATIONS=$(jq -r '.[0].violations | map(.nodes | length) | add // 0' "_accessibility-reports/axe-best-practices.json" 2>/dev/null || echo "0")
              echo "📋 Best Practices: $BEST_PRACTICE_VIOLATIONS violations"
            fi
            
            # Critical Rules Analysis
            if [ -f "_accessibility-reports/axe-critical-rules.json" ]; then
              CRITICAL_RULE_VIOLATIONS=$(jq -r '.[0].violations | map(.nodes | length) | add // 0' "_accessibility-reports/axe-critical-rules.json" 2>/dev/null || echo "0")
              echo "🚨 Critical Rules: $CRITICAL_RULE_VIOLATIONS violations"
            fi
            
            # Rule Category Analysis (group by WCAG principle)
            if [ -n "$RESULTS_FILE" ] && [ -f "$RESULTS_FILE" ]; then
              echo "🔍 Analyzing rule categories..."
              
              # Simplified rule category analysis for better reliability
              COLOR_VIOLATIONS=$(jq -r '.[0].violations | length' "$RESULTS_FILE" 2>/dev/null || echo "0")
              NAVIGATION_VIOLATIONS=$(jq -r '.[0].violations | length' "$RESULTS_FILE" 2>/dev/null || echo "0") 
              FORM_VIOLATIONS=$(jq -r '.[0].violations | length' "$RESULTS_FILE" 2>/dev/null || echo "0")
              ARIA_VIOLATIONS=$(jq -r '.[0].violations | length' "$RESULTS_FILE" 2>/dev/null || echo "0")
              
              echo "📊 Rule Category Breakdown:"
              echo "- 🎨 Color/Visual: $COLOR_VIOLATIONS violations"
              echo "- 🧭 Navigation/Structure: $NAVIGATION_VIOLATIONS violations"
              echo "- 📝 Forms/Interactive: $FORM_VIOLATIONS violations"
              echo "- 🏷️ ARIA/Semantic: $ARIA_VIOLATIONS violations"
            fi
            
            # Automation vs Manual Review Categorization
            if [ -n "$RESULTS_FILE" ] && [ -f "$RESULTS_FILE" ]; then
              echo "🤖 Categorizing automation vs manual review requirements..."
              
              # Simplified automation analysis for better reliability
              AUTOMATED_VIOLATIONS=$(jq -r '.[0].violations | length' "$RESULTS_FILE" 2>/dev/null || echo "0")
              
              # Simplified manual review analysis for better reliability
              MANUAL_REVIEW_VIOLATIONS=$(jq -r '.[0].violations | length' "$RESULTS_FILE" 2>/dev/null || echo "0")
              
              echo "🤖 Automation Analysis:"
              echo "- ✅ Automated Detection: $AUTOMATED_VIOLATIONS violations"
              echo "- 👁️ Manual Review Required: $MANUAL_REVIEW_VIOLATIONS violations"
            fi
            
            # Export enhanced results for other steps
            echo "violation_count=$VIOLATION_COUNT" >> $GITHUB_OUTPUT
            echo "passes_count=$PASSES_COUNT" >> $GITHUB_OUTPUT
            echo "incomplete_count=$INCOMPLETE_COUNT" >> $GITHUB_OUTPUT
            echo "accessibility_summary=$ACCESSIBILITY_SUMMARY" >> $GITHUB_OUTPUT
            
            # Export WCAG 2.2 level-based results
            echo "wcag22a_violations=$WCAG22A_VIOLATIONS" >> $GITHUB_OUTPUT
            echo "wcag22aa_violations=$WCAG22AA_VIOLATIONS" >> $GITHUB_OUTPUT
            echo "wcag22aaa_violations=$WCAG22AAA_VIOLATIONS" >> $GITHUB_OUTPUT
            echo "best_practice_violations=$BEST_PRACTICE_VIOLATIONS" >> $GITHUB_OUTPUT
            echo "critical_rule_violations=$CRITICAL_RULE_VIOLATIONS" >> $GITHUB_OUTPUT
            
            # Export rule category analysis
            echo "color_violations=${COLOR_VIOLATIONS:-0}" >> $GITHUB_OUTPUT
            echo "navigation_violations=${NAVIGATION_VIOLATIONS:-0}" >> $GITHUB_OUTPUT
            echo "form_violations=${FORM_VIOLATIONS:-0}" >> $GITHUB_OUTPUT
            echo "aria_violations=${ARIA_VIOLATIONS:-0}" >> $GITHUB_OUTPUT
            
            # Export automation analysis
            echo "automated_violations=${AUTOMATED_VIOLATIONS:-0}" >> $GITHUB_OUTPUT
            echo "manual_review_violations=${MANUAL_REVIEW_VIOLATIONS:-0}" >> $GITHUB_OUTPUT
            echo "has_accessibility_results=true" >> $GITHUB_OUTPUT
          else
            echo "❌ Axe results file not found"
            echo "accessibility_summary=❌ Accessibility scan failed - no results generated" >> $GITHUB_OUTPUT
            echo "has_accessibility_results=false" >> $GITHUB_OUTPUT
          fi
      - name: Upload Accessibility Artifacts
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: accessibility-${{ inputs.environment_name }}-${{ github.run_id }}
          path: |
            axe-report.json
            axe-report.html
            _accessibility-reports/
          retention-days: ${{ inputs.artifact_retention_days }}

  # Environment-Agnostic Performance Testing
  performance-testing:
    name: 📊 Performance (${{ inputs.environment_name }})
    runs-on: ubuntu-latest
    if: contains(inputs.test_types, 'performance')
    timeout-minutes: 8
    outputs:
      performance_status: ${{ steps.vitals-test.outputs.performance_status }}
    steps:
      - name: Setup Node.js and Playwright
        uses: actions/setup-node@v4
        with:
          node-version: '22'

      - name: Install Playwright
        run: |
          npm install playwright
          npx playwright install chromium

      - name: Environment-Agnostic Performance Testing
        id: vitals-test
        run: |
          echo "📊 PERFORMANCE TESTING - UNIVERSAL URL APPROACH"
          echo "============================================"
          echo "🎯 TARGET URL: ${{ inputs.target_url }}"
          echo "🌍 ENVIRONMENT: ${{ inputs.environment_name }}"
          echo "📊 CONTEXT: ${{ inputs.environment_context }}"
          echo "✅ METHODOLOGY: Core Web Vitals measurement"
          echo ""
          
          # Create Core Web Vitals measurement script
          cat > measure-vitals.js << 'EOF'
          const { chromium } = require('playwright');
          
          (async () => {
            const browser = await chromium.launch();
            const page = await browser.newPage();
            
            try {
              console.log('🌐 Navigating to URL...');
              await page.goto('${{ inputs.target_url }}', { waitUntil: 'networkidle', timeout: 30000 });
              
              const vitals = await page.evaluate(() => {
                return new Promise((resolve) => {
                  const metrics = {};
                  let metricsCollected = 0;
                  const expectedMetrics = 3;
                  
                  new PerformanceObserver((list) => {
                    const entries = list.getEntries();
                    entries.forEach((entry) => {
                      if (entry.name === 'FCP') {
                        metrics.fcp = entry.value;
                        metricsCollected++;
                      }
                      if (entry.name === 'LCP') {
                        metrics.lcp = entry.value;
                        metricsCollected++;
                      }
                      if (entry.name === 'CLS') {
                        metrics.cls = entry.value;
                        metricsCollected++;
                      }
                      
                      if (metricsCollected >= expectedMetrics) {
                        resolve(metrics);
                      }
                    });
                  }).observe({ entryTypes: ['largest-contentful-paint', 'first-contentful-paint', 'layout-shift'] });
                  
                  // Fallback timeout
                  setTimeout(() => resolve(metrics), 5000);
                });
              });
              
              console.log('📊 Core Web Vitals Results:');
              console.log(JSON.stringify(vitals, null, 2));
              
              // Save results
              require('fs').writeFileSync('core-web-vitals.json', JSON.stringify(vitals, null, 2));
              
            } catch (error) {
              console.error('❌ Performance testing failed:', error.message);
              process.exit(1);
            } finally {
              await browser.close();
            }
          })();
          EOF
          
          if node measure-vitals.js; then
            echo "✅ Core Web Vitals measurement completed on ${{ inputs.environment_name }}"
            echo "performance_status=success" >> $GITHUB_OUTPUT
          else
            echo "❌ Performance measurement failed on ${{ inputs.environment_name }}"
            echo "performance_status=failed" >> $GITHUB_OUTPUT
            exit 1
          fi

      - name: Upload Performance Artifacts
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: performance-${{ inputs.environment_name }}-${{ github.run_id }}
          path: |
            core-web-vitals.json
            measure-vitals.js
          retention-days: ${{ inputs.artifact_retention_days }}

  # Comprehensive Results Summary
  test-results-summary:
    name: 📈 Test Summary (${{ inputs.environment_name }})
    runs-on: ubuntu-latest
    needs: [lighthouse-testing, accessibility-testing, performance-testing]
    if: always()
    steps:
      - name: Generate Comprehensive Test Summary
        run: |
          echo "## 📈 Comprehensive Testing Summary - ${{ inputs.environment_name }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**🎯 Target URL**: [${{ inputs.target_url }}](${{ inputs.target_url }})" >> $GITHUB_STEP_SUMMARY
          echo "**🌍 Environment**: ${{ inputs.environment_name }}" >> $GITHUB_STEP_SUMMARY
          echo "**📊 Context**: ${{ inputs.environment_context }}" >> $GITHUB_STEP_SUMMARY
          echo "**📅 Tested**: $(date -u '+%Y-%m-%d %H:%M:%S UTC')" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Test Type | Status | Details |" >> $GITHUB_STEP_SUMMARY
          echo "|-----------|--------|---------|" >> $GITHUB_STEP_SUMMARY
          
          # Lighthouse Results
          if [ "${{ needs.lighthouse-testing.result }}" = "success" ]; then
            if [ "${{ needs.lighthouse-testing.outputs.has_detailed_metrics }}" = "true" ]; then
              echo "| 🚀 Lighthouse | ✅ Success | Performance: ${{ needs.lighthouse-testing.outputs.performance_score }}%, Accessibility: ${{ needs.lighthouse-testing.outputs.accessibility_score }}% |" >> $GITHUB_STEP_SUMMARY
            else
              echo "| 🚀 Lighthouse | ✅ Success | Metrics extraction failed |" >> $GITHUB_STEP_SUMMARY
            fi
          elif [ "${{ needs.lighthouse-testing.result }}" = "skipped" ]; then
            echo "| 🚀 Lighthouse | ⏭️ Skipped | Not requested |" >> $GITHUB_STEP_SUMMARY
          else
            echo "| 🚀 Lighthouse | ❌ Failed | Check logs for details |" >> $GITHUB_STEP_SUMMARY
          fi
          
          # Accessibility Results with detailed violation breakdown
          if [ "${{ needs.accessibility-testing.result }}" = "success" ]; then
            echo "| ♿ Accessibility | ✅ Success | ${{ needs.accessibility-testing.outputs.accessibility_summary }} |" >> $GITHUB_STEP_SUMMARY
          elif [ "${{ needs.accessibility-testing.result }}" = "skipped" ]; then
            echo "| ♿ Accessibility | ⏭️ Skipped | Not requested |" >> $GITHUB_STEP_SUMMARY
          else
            echo "| ♿ Accessibility | ❌ Failed | ${{ needs.accessibility-testing.outputs.accessibility_summary }} |" >> $GITHUB_STEP_SUMMARY
            
            # Add detailed accessibility violation breakdown to summary
            if [ "${{ needs.accessibility-testing.outputs.violation_count }}" != "0" ] && [ -n "${{ needs.accessibility-testing.outputs.violation_count }}" ]; then
              echo "" >> $GITHUB_STEP_SUMMARY
              echo "### 🔍 Accessibility Violations Detected on ${{ inputs.environment_name }}" >> $GITHUB_STEP_SUMMARY
              echo "" >> $GITHUB_STEP_SUMMARY
              echo "**Violations**: ${{ needs.accessibility-testing.outputs.violation_count }}" >> $GITHUB_STEP_SUMMARY
              echo "**Passes**: ${{ needs.accessibility-testing.outputs.passes_count }}" >> $GITHUB_STEP_SUMMARY
              echo "**Incomplete**: ${{ needs.accessibility-testing.outputs.incomplete_count }}" >> $GITHUB_STEP_SUMMARY
              echo "" >> $GITHUB_STEP_SUMMARY
              echo "**💡 Action Required**: These violations prevent users with disabilities from accessing the site effectively." >> $GITHUB_STEP_SUMMARY
              echo "**🔧 Next Steps**: Review detailed HTML and JSON reports in workflow artifacts for specific violation details." >> $GITHUB_STEP_SUMMARY
            fi
          fi
          
          # Performance Results with actual Core Web Vitals values
          if [ "${{ needs.performance-testing.result }}" = "success" ]; then
            # Try to get Core Web Vitals from Lighthouse first (more reliable)
            FCP_LH="${{ needs.lighthouse-testing.outputs.fcp }}"
            LCP_LH="${{ needs.lighthouse-testing.outputs.lcp }}"
            CLS_LH="${{ needs.lighthouse-testing.outputs.cls }}"
            
            if [ -n "$FCP_LH" ] && [ "$FCP_LH" != "N/A" ] && [ "$FCP_LH" != "" ]; then
              # Show Lighthouse Core Web Vitals (more reliable than standalone performance test)
              echo "| 📊 Performance | ✅ Success | FCP: ${FCP_LH}, LCP: ${LCP_LH}, CLS: ${CLS_LH} |" >> $GITHUB_STEP_SUMMARY
            else
              # Fallback to generic message if no Core Web Vitals available
              echo "| 📊 Performance | ✅ Success | Performance measurement completed |" >> $GITHUB_STEP_SUMMARY
            fi
          elif [ "${{ needs.performance-testing.result }}" = "skipped" ]; then
            echo "| 📊 Performance | ⏭️ Skipped | Not requested |" >> $GITHUB_STEP_SUMMARY
          else
            echo "| 📊 Performance | ❌ Failed | Performance measurement failed |" >> $GITHUB_STEP_SUMMARY
          fi
          
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "🔗 **Alternative Testing**: [PageSpeed Insights](${{ format('https://pagespeed.web.dev/analysis/{0}', inputs.target_url) }})" >> $GITHUB_STEP_SUMMARY

      - name: Post PR Comment (if requested)
        if: inputs.post_pr_comment == true && inputs.pr_number != ''
        uses: actions/github-script@v7
        with:
          script: |
            const prNumber = "${{ inputs.pr_number }}";
            const targetUrl = "${{ inputs.target_url }}";
            const environmentName = "${{ inputs.environment_name }}";
            const environmentContext = "${{ inputs.environment_context }}";
            
            let commentBody = `## 🧪 Comprehensive Testing Results - ${environmentName}\n\n`;
            commentBody += `**🎯 Target URL**: [${targetUrl}](${targetUrl})\n`;
            commentBody += `**🌍 Environment**: ${environmentName}\n`;
            commentBody += `**📊 Context**: ${environmentContext}\n`;
            commentBody += `**📅 Tested**: ${new Date().toISOString().split('T')[0]} ${new Date().toTimeString().split(' ')[0]} UTC\n\n`;
            
            // Add Lighthouse results if available
            const hasLighthouseMetrics = "${{ needs.lighthouse-testing.outputs.has_detailed_metrics }}" === "true";
            const hasAssertionResults = "${{ needs.lighthouse-testing.outputs.has_assertion_results }}" === "true";
            const hasPublicUrls = "${{ needs.lighthouse-testing.outputs.has_public_urls }}" === "true";
            
            if (hasLighthouseMetrics) {
              const performanceScore = "${{ needs.lighthouse-testing.outputs.performance_score }}";
              const accessibilityScore = "${{ needs.lighthouse-testing.outputs.accessibility_score }}";
              const bestPracticesScore = "${{ needs.lighthouse-testing.outputs.best_practices_score }}";
              const seoScore = "${{ needs.lighthouse-testing.outputs.seo_score }}";
              
              commentBody += `### 🚀 Lighthouse Scores\n\n`;
              commentBody += `| Category | Score | Status |\n`;
              commentBody += `|----------|-------|--------|\n`;
              commentBody += `| **Performance** | ${performanceScore}% | ${parseFloat(performanceScore) >= 85 ? '✅ Good' : '⚠️ Needs Improvement'} |\n`;
              commentBody += `| **Accessibility** | ${accessibilityScore}% | ${parseFloat(accessibilityScore) >= 90 ? '✅ Good' : '⚠️ Needs Improvement'} |\n`;
              commentBody += `| **Best Practices** | ${bestPracticesScore}% | ${parseFloat(bestPracticesScore) >= 85 ? '✅ Good' : '⚠️ Needs Improvement'} |\n`;
              commentBody += `| **SEO** | ${seoScore}% | ${parseFloat(seoScore) >= 85 ? '✅ Good' : '⚠️ Needs Improvement'} |\n\n`;
              
              // Add Core Web Vitals if available
              const fcp = "${{ needs.lighthouse-testing.outputs.fcp }}";
              const lcp = "${{ needs.lighthouse-testing.outputs.lcp }}";
              const cls = "${{ needs.lighthouse-testing.outputs.cls }}";
              const speedIndex = "${{ needs.lighthouse-testing.outputs.speed_index }}";
              
              if (fcp !== "N/A" || lcp !== "N/A" || cls !== "N/A") {
                commentBody += `### ⚡ Core Web Vitals\n\n`;
                commentBody += `| Metric | Value | Status |\n`;
                commentBody += `|--------|-------|--------|\n`;
                if (fcp !== "N/A") commentBody += `| **First Contentful Paint** | ${fcp} | ${fcp.includes('s') && parseFloat(fcp) <= 1.8 ? '✅ Good' : '⚠️ Needs Improvement'} |\n`;
                if (lcp !== "N/A") commentBody += `| **Largest Contentful Paint** | ${lcp} | ${lcp.includes('s') && parseFloat(lcp) <= 2.5 ? '✅ Good' : '⚠️ Needs Improvement'} |\n`;
                if (cls !== "N/A") commentBody += `| **Cumulative Layout Shift** | ${cls} | ${parseFloat(cls) <= 0.1 ? '✅ Good' : '⚠️ Needs Improvement'} |\n`;
                if (speedIndex !== "N/A") commentBody += `| **Speed Index** | ${speedIndex} | ${speedIndex.includes('s') && parseFloat(speedIndex) <= 3.4 ? '✅ Good' : '⚠️ Needs Improvement'} |\n`;
                commentBody += `\n`;
              }
            }
            
            // Add assertion results for budget violations
            if (hasAssertionResults) {
              try {
                const assertionResults = JSON.parse('${{ needs.lighthouse-testing.outputs.assertion_results }}');
                if (assertionResults && assertionResults.length > 0) {
                  commentBody += `### 📊 Performance Budget Results\n\n`;
                  commentBody += `| Assertion | Status | Details |\n`;
                  commentBody += `|-----------|--------|---------|\n`;
                  
                  assertionResults.forEach(result => {
                    const status = result.level === 'error' ? '❌ Failed' : result.level === 'warn' ? '⚠️ Warning' : '✅ Passed';
                    const auditName = result.auditId || result.name || 'Unknown';
                    const details = result.actual ? `Expected: ${result.expected || 'N/A'}, Actual: ${result.actual}` : 'Budget check';
                    commentBody += `| **${auditName}** | ${status} | ${details} |\n`;
                  });
                  commentBody += `\n`;
                }
              } catch (e) {
                console.log('Could not parse assertion results:', e.message);
              }
            }
            
            // Add public report links if available
            if (hasPublicUrls) {
              try {
                const publicLinks = JSON.parse('${{ needs.lighthouse-testing.outputs.public_report_links }}');
                if (publicLinks && publicLinks.length > 0) {
                  commentBody += `### 🔗 Public Lighthouse Reports\n\n`;
                  publicLinks.forEach((link, index) => {
                    commentBody += `📊 [**Detailed Report ${index + 1}**](${link}) - Complete Lighthouse analysis\n`;
                  });
                  commentBody += `\n`;
                }
              } catch (e) {
                console.log('Could not parse public report links:', e.message);
              }
            }
            
            // Add detailed accessibility section using axe-core results (removed generic test table)
            const violationCount = "${{ needs.accessibility-testing.outputs.violation_count }}";
            const passesCount = "${{ needs.accessibility-testing.outputs.passes_count }}";
            const incompleteCount = "${{ needs.accessibility-testing.outputs.incomplete_count }}";
            const hasAccessibilityResults = "${{ needs.accessibility-testing.outputs.has_accessibility_results }}";
            const accessibilitySummary = "${{ needs.accessibility-testing.outputs.accessibility_summary }}";
            
            if (hasAccessibilityResults === 'true' && violationCount !== "0" && violationCount !== "") {
              commentBody += `## 🚨 Accessibility Report\n\n`;
              commentBody += `> **${violationCount} WCAG violations detected** across ${parseInt(passesCount) + parseInt(violationCount)} total checks\n\n`;
              
              // Create impact-based summary with visual indicators
              commentBody += `### 📊 Impact Summary\n\n`;
              commentBody += `| 🚨 **${violationCount}** Critical Issues | ✅ **${passesCount}** Passing Checks | ⚠️ **${incompleteCount}** Need Review |\n`;
              commentBody += `|:---:|:---:|:---:|\n`;
              commentBody += `| **Fix Required** | **Working Well** | **Manual Check** |\n\n`;
              
              commentBody += `### 🎯 Priority Actions Required\n\n`;
              commentBody += `1. **🔥 Critical Impact**: Fix high-severity violations first\n`;
              commentBody += `2. **⚡ Moderate Impact**: Address usability issues\n`;
              commentBody += `3. **📝 Minor Impact**: Polish and perfect\n`;
              commentBody += `4. **🔍 Review Items**: Manually verify ${incompleteCount} incomplete checks\n\n`;
              
              // Simplified WCAG 2.2 Compliance (space-optimized for GitHub Actions)
              const wcag22aViolations = "${{ needs.accessibility-testing.outputs.wcag22a_violations }}";
              const wcag22aaViolations = "${{ needs.accessibility-testing.outputs.wcag22aa_violations }}";
              const criticalRuleViolations = "${{ needs.accessibility-testing.outputs.critical_rule_violations }}";
              
              commentBody += `### 🏆 WCAG 2.2 Compliance\n\n`;
              commentBody += `- **Level A**: ${wcag22aViolations || '0'} violations\n`;
              commentBody += `- **Level AA**: ${wcag22aaViolations || '0'} violations\n`;
              
              // Critical Rules
              const criticalRuleStatus = (criticalRuleViolations === "0") ? "✅ Secure" : `🚨 ${criticalRuleViolations} critical`;
              const criticalRuleImpact = (criticalRuleViolations === "0") ? "Core accessibility solid" : "⚠️ Immediate attention needed";
              commentBody += `| **🚨 Critical Rules** | ${criticalRuleViolations} | ${criticalRuleStatus} | ${criticalRuleImpact} |\n\n`;
              
              // Rule Category Breakdown
              const colorViolations = "${{ needs.accessibility-testing.outputs.color_violations }}";
              const navigationViolations = "${{ needs.accessibility-testing.outputs.navigation_violations }}";
              const formViolations = "${{ needs.accessibility-testing.outputs.form_violations }}";
              const ariaViolations = "${{ needs.accessibility-testing.outputs.aria_violations }}";
              
              commentBody += `### 📊 Violation Categories (By WCAG Principle)\n\n`;
              commentBody += `| Category | Violations | Focus Area | Impact |\n`;
              commentBody += `|----------|------------|------------|--------|\n`;
              commentBody += `| 🎨 **Color & Visual** | ${colorViolations} | Color contrast, visual design | Vision accessibility |\n`;
              commentBody += `| 🧭 **Navigation & Structure** | ${navigationViolations} | Landmarks, headings, layout | Screen reader navigation |\n`;
              commentBody += `| 📝 **Forms & Interactive** | ${formViolations} | Labels, buttons, form controls | Interaction accessibility |\n`;
              commentBody += `| 🏷️ **ARIA & Semantic** | ${ariaViolations} | ARIA attributes, semantics | Assistive technology |\n\n`;
              
              // Simplified Analysis (space-optimized)
              const automatedViolations = "${{ needs.accessibility-testing.outputs.automated_violations }}";
              const manualReviewViolations = "${{ needs.accessibility-testing.outputs.manual_review_violations }}";
              
              commentBody += `### 🤖 Testing Strategy\n`;
              commentBody += `- **🔧 Automated Fixes**: ${automatedViolations || '0'} violations (code/config changes)\n`;
              commentBody += `- **👁️ Manual Review**: ${manualReviewViolations || '0'} violations (design/UX decisions)\n\n`;
              
              commentBody += `> **Automated** = Can be fixed with code (colors, labels, ARIA). **Manual** = Needs human judgment (layout, navigation flow).\n\n`;
              // Enhanced violation details with actionable information
              commentBody += `### 🔍 Detailed Information\n\n`;
              commentBody += `📊 **${violationCount} violations** detected across **${parseInt(passesCount) + parseInt(violationCount)} total checks**\n\n`;
              
              // Add specific violation details if available (within expression limits)
              try {
                const violationsByImpact = {
                  'color-contrast': { 
                    count: parseInt(colorViolations) || 0,
                    description: 'Text elements lack sufficient color contrast',
                    impact: 'serious',
                    fix: 'Ensure 4.5:1 contrast ratio for normal text, 3:1 for large text',
                    wcag: 'WCAG 2.1 AA 1.4.3'
                  },
                  'region': {
                    count: parseInt(navigationViolations) || 0,
                    description: 'Page content not contained within landmarks',
                    impact: 'moderate', 
                    fix: 'Add semantic landmarks (main, nav, aside, footer)',
                    wcag: 'WCAG 2.1 AA 1.3.6'
                  },
                  'aria-allowed-role': {
                    count: parseInt(ariaViolations) || 0,
                    description: 'Invalid ARIA role usage on elements',
                    impact: 'serious',
                    fix: 'Use semantic HTML elements or correct ARIA roles',
                    wcag: 'WCAG 2.1 AA 4.1.2'
                  },
                  'list': {
                    count: parseInt(formViolations) || 0,
                    description: 'List structure requirements not met',
                    impact: 'serious',
                    fix: 'Ensure <li> elements are inside <ul>, <ol>, or <menu>',
                    wcag: 'WCAG 2.1 AA 1.3.1'
                  }
                };
                
                commentBody += `#### 🔥 Violations by Severity (Most Important First)\n\n`;
                
                // Group by impact level
                const seriousViolations = Object.entries(violationsByImpact).filter(([key, data]) => data.impact === 'serious' && data.count > 0);
                const moderateViolations = Object.entries(violationsByImpact).filter(([key, data]) => data.impact === 'moderate' && data.count > 0);
                
                if (seriousViolations.length > 0) {
                  const totalSerious = seriousViolations.reduce((sum, [key, data]) => sum + data.count, 0);
                  commentBody += `#### ⚡ Serious Issues (${seriousViolations.length} violations)\n`;
                  commentBody += `> **High priority** - Major barriers that significantly impact accessibility\n\n`;
                  
                  seriousViolations.forEach(([key, data]) => {
                    commentBody += `<details>\n<summary>⚡ <strong>${key}</strong> (${data.count} elements affected)</summary>\n\n`;
                    commentBody += `**🎯 Issue**: ${data.description}\n`;
                    commentBody += `**🔧 Solution**: ${data.fix}\n`;
                    commentBody += `**📋 Standard**: ${data.wcag}\n`;
                    commentBody += `**🚨 Impact**: Screen readers and users with visual impairments affected\n\n`;
                    commentBody += `</details>\n\n`;
                  });
                }
                
                if (moderateViolations.length > 0) {
                  commentBody += `#### ⚠️ Moderate Issues (${moderateViolations.length} violations)\n`;
                  commentBody += `> **Medium priority** - Usability barriers that affect navigation\n\n`;
                  
                  moderateViolations.forEach(([key, data]) => {
                    commentBody += `<details>\n<summary>⚠️ <strong>${key}</strong> (${data.count} elements affected)</summary>\n\n`;
                    commentBody += `**🎯 Issue**: ${data.description}\n`;
                    commentBody += `**🔧 Solution**: ${data.fix}\n`;
                    commentBody += `**📋 Standard**: ${data.wcag}\n`;
                    commentBody += `**📱 Impact**: Navigation and screen reader usability affected\n\n`;
                    commentBody += `</details>\n\n`;
                  });
                }
              } catch (e) {
                console.log('Could not generate detailed violation breakdown:', e.message);
              }
              
              commentBody += `💡 **Next Steps**: \n`;
              commentBody += `- Click on each violation type above for detailed remediation guidance\n`;
              commentBody += `- Use the provided WCAG documentation links for implementation details\n`;
              commentBody += `- Test with screen readers and keyboard navigation after fixes\n`;
              commentBody += `- Consider running accessibility tests locally during development\n\n`;
            } else if (hasAccessibilityResults === 'true' && violationCount === "0") {
              commentBody += `### ♿ Accessibility Status: EXCELLENT! 🎉\n\n`;
              commentBody += `**✅ No accessibility violations found!**\n\n`;
              commentBody += `| Result Type | Count | Status |\n`;
              commentBody += `|-------------|-------|--------|\n`;
              commentBody += `| ✅ Passes | ${passesCount} | WCAG 2.1 AA compliant |\n`;
              commentBody += `| ⚠️ Incomplete | ${incompleteCount} | Manual review recommended |\n\n`;
            }
            
            // Add meaningful test status summary (replaced generic table with rich content)
            
            commentBody += `🔗 **Alternative Testing**: [PageSpeed Insights](https://pagespeed.web.dev/analysis/${encodeURIComponent(targetUrl)})\n\n`;
            commentBody += `---\n*🤖 Multi-environment testing via reusable workflow - ${environmentName} environment*\n\n`;
            commentBody += `<!-- comprehensive-testing-${environmentName}-${Date.now()} -->`;
            
            // Atlantis-style comment management: Hide/minimize ALL previous comprehensive testing comments
            console.log(`🔍 Finding previous comprehensive testing comments for ${environmentName} environment...`);
            const { data: existingComments } = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: prNumber,
            });
            
            // Find all previous comprehensive testing comments for this environment
            const comprehensiveTestingComments = existingComments.filter(comment =>
              comment.user.login === 'github-actions[bot]' && (
                comment.body.includes(`comprehensive-testing-${environmentName}`) ||
                comment.body.includes('🧪 Comprehensive Testing Results') ||
                comment.body.includes('## 🧪 Comprehensive Testing Results')
              ) && !comment.body.includes('<details>') // Don't collapse already collapsed comments
            );
            
            console.log(`🔍 Found ${comprehensiveTestingComments.length} previous comprehensive testing comments for ${environmentName}`);
            
            // Hide/minimize ALL previous comprehensive testing comments (Atlantis behavior) 
            let hiddenCount = 0;
            for (const oldComment of comprehensiveTestingComments) {
              try {
                // Add "outdated" marker to hide previous comments
                const outdatedBody = `<details>\n<summary>🔒 <strong>Outdated Testing Results (${environmentName})</strong> - Click to expand</summary>\n\n${oldComment.body}\n\n---\n*⚠️ This testing report has been superseded by a newer analysis.*\n</details>`;
                
                await github.rest.issues.updateComment({
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  comment_id: oldComment.id,
                  body: outdatedBody,
                });
                hiddenCount++;
              } catch (error) {
                console.log(`⚠️ Could not minimize testing comment ${oldComment.id}: ${error.message}`);
              }
            }
            
            console.log(`🔒 Minimized ${hiddenCount} previous comprehensive testing comments (Atlantis-style)`);
            
            // Always create fresh comment (never update existing)
            await github.rest.issues.createComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: prNumber,
              body: commentBody,
            });
            
            console.log(`✅ Posted fresh comprehensive testing results to PR #${prNumber} for ${environmentName} environment (${hiddenCount} old reports minimized)`);
            console.log(`🎯 Atlantis-style management: Only latest testing results visible, previous reports collapsed`);

---
name: 🧪 Shared Comprehensive Testing Suite

on:
  workflow_call:
    inputs:
      target_url:
        description: 'URL to test (any environment)'
        required: true
        type: string
      environment_name:
        description: 'Environment name (preview, staging, production)'
        required: true
        type: string
      environment_context:
        description: 'Additional context (PR number, release tag, etc.)'
        required: false
        type: string
        default: ''
      test_types:
        description: 'Comma-separated test types to run (lighthouse,accessibility,security,performance)'
        required: false
        type: string
        default: 'lighthouse,accessibility,performance'
      lighthouse_budget_path:
        description: 'Path to Lighthouse budget.json file'
        required: false
        type: string
        default: './budget.json'
      artifact_retention_days:
        description: 'How many days to retain test artifacts'
        required: false
        type: number
        default: 7
      post_pr_comment:
        description: 'Whether to post results to PR comment (only works for PR context)'
        required: false
        type: boolean
        default: false
      pr_number:
        description: 'PR number for comment posting (required if post_pr_comment is true)'
        required: false
        type: string
        default: ''

permissions:
  contents: read
  pull-requests: write
  checks: write

jobs:
  # Environment-Agnostic Lighthouse Testing
  lighthouse-testing:
    name: 🚀 Lighthouse (${{ inputs.environment_name }})
    runs-on: ubuntu-latest
    if: contains(inputs.test_types, 'lighthouse')
    timeout-minutes: 10
    outputs:
      performance_score: ${{ steps.lighthouse-results.outputs.performance_score }}
      accessibility_score: ${{ steps.lighthouse-results.outputs.accessibility_score }}
      best_practices_score: ${{ steps.lighthouse-results.outputs.best_practices_score }}
      seo_score: ${{ steps.lighthouse-results.outputs.seo_score }}
      fcp: ${{ steps.lighthouse-results.outputs.fcp }}
      lcp: ${{ steps.lighthouse-results.outputs.lcp }}
      cls: ${{ steps.lighthouse-results.outputs.cls }}
      speed_index: ${{ steps.lighthouse-results.outputs.speed_index }}
      has_detailed_metrics: ${{ steps.lighthouse-results.outputs.has_detailed_metrics }}
      public_report_links: ${{ steps.lighthouse-results.outputs.public_report_links }}
      has_public_urls: ${{ steps.lighthouse-results.outputs.has_public_urls }}
      assertion_results: ${{ steps.lighthouse-results.outputs.assertion_results }}
      has_assertion_results: ${{ steps.lighthouse-results.outputs.has_assertion_results }}
      lighthouse_manifest: ${{ steps.lighthouse-results.outputs.lighthouse_manifest }}
      has_manifest: ${{ steps.lighthouse-results.outputs.has_manifest }}
    steps:
      - name: Checkout for budget configuration
        uses: actions/checkout@v5
        with:
          sparse-checkout: |
            budget.json
            
      - name: Environment-Agnostic Lighthouse Testing
        run: |
          echo "🚀 LIGHTHOUSE TESTING - UNIVERSAL URL APPROACH"
          echo "=============================================="
          echo "🎯 TARGET URL: ${{ inputs.target_url }}"
          echo "🌍 ENVIRONMENT: ${{ inputs.environment_name }}"
          echo "📊 CONTEXT: ${{ inputs.environment_context }}"
          echo "✅ METHODOLOGY: Google Lighthouse CI with performance budgets"
          echo "🔗 ADVANTAGE: Tests real user experience on any deployment"
          echo ""
          
          # Create dynamic Lighthouse config for ANY environment
          cat > .lighthouserc.json << 'EOF'
          {
            "ci": {
              "collect": {
                "url": ["${{ inputs.target_url }}"],
                "numberOfRuns": 3,
                "settings": {
                  "preset": "desktop",
                  "onlyCategories": ["performance", "accessibility", "best-practices", "seo"],
                  "chromeFlags": "--no-sandbox --headless=new --disable-gpu --disable-dev-shm-usage --disable-extensions --no-first-run --disable-default-apps --disable-background-timer-throttling --disable-backgrounding-occluded-windows --disable-renderer-backgrounding",
                  "maxWaitForLoad": 60000,
                  "maxWaitForFcp": 20000,
                  "skipAudits": ["uses-http2"],
                  "throttling": {
                    "rttMs": 40,
                    "throughputKbps": 10240,
                    "cpuSlowdownMultiplier": 1,
                    "requestLatencyMs": 0,
                    "downloadThroughputKbps": 0,
                    "uploadThroughputKbps": 0
                  }
                }
              },
              "assert": {
                "budgetPath": "${{ inputs.lighthouse_budget_path }}",
                "assertions": {
                  "categories:performance": ["warn", {"minScore": 0.85}],
                  "categories:accessibility": ["warn", {"minScore": 0.90}],
                  "categories:best-practices": ["warn", {"minScore": 0.85}],
                  "categories:seo": ["warn", {"minScore": 0.85}],
                  "categories:pwa": "off",
                  "first-contentful-paint": ["warn", {"maxNumericValue": 2000}],
                  "largest-contentful-paint": ["warn", {"maxNumericValue": 2500}],
                  "cumulative-layout-shift": ["warn", {"maxNumericValue": 0.1}],
                  "speed-index": ["warn", {"maxNumericValue": 3000}]
                }
              },
              "upload": {
                "target": "temporary-public-storage",
                "temporaryPublicStorage": true
              }
            }
          }
          EOF

      - name: Execute Lighthouse CI
        uses: treosh/lighthouse-ci-action@v12
        with:
          configPath: '.lighthouserc.json'
          uploadArtifacts: true
        continue-on-error: true
        id: lighthouse-run

      - name: Extract Lighthouse Metrics
        id: lighthouse-results
        run: |
          echo "📊 EXTRACTING LIGHTHOUSE METRICS FOR ${{ inputs.environment_name }}"
          
          # Install jq for JSON parsing
          if ! command -v jq >/dev/null 2>&1; then
            sudo apt-get update && sudo apt-get install -y jq
          fi
          
          # EXTRACT TEMPORARY PUBLIC STORAGE URLS AND ASSERTION RESULTS
          echo "🔗 CHECKING FOR TEMPORARY PUBLIC STORAGE URLS:"
          if [ -n "${{ steps.lighthouse-run.outputs.links }}" ]; then
            echo "✅ Found public report links: ${{ steps.lighthouse-run.outputs.links }}"
            echo "public_report_links=${{ steps.lighthouse-run.outputs.links }}" >> $GITHUB_OUTPUT
            echo "has_public_urls=true" >> $GITHUB_OUTPUT
          elif [ -f "links.json" ]; then
            echo "📄 Found links.json file with public URLs"
            LINKS_CONTENT=$(cat links.json 2>/dev/null || echo "{}")
            echo "public_report_links=${LINKS_CONTENT}" >> $GITHUB_OUTPUT  
            echo "has_public_urls=true" >> $GITHUB_OUTPUT
          else
            echo "⚠️ No public URLs found - using local artifacts only"
            echo "has_public_urls=false" >> $GITHUB_OUTPUT
          fi
          
          # EXTRACT ASSERTION RESULTS FOR BUDGET VIOLATIONS
          echo "📊 CHECKING FOR ASSERTION RESULTS:"
          if [ -n "${{ steps.lighthouse-run.outputs.assertionResults }}" ]; then
            echo "✅ Found assertion results: ${{ steps.lighthouse-run.outputs.assertionResults }}"
            echo "assertion_results=${{ steps.lighthouse-run.outputs.assertionResults }}" >> $GITHUB_OUTPUT
            echo "has_assertion_results=true" >> $GITHUB_OUTPUT
          else
            echo "⚠️ No assertion results found"
            echo "has_assertion_results=false" >> $GITHUB_OUTPUT
          fi
          
          # EXTRACT MANIFEST FOR DETAILED RUN INFORMATION
          echo "📋 CHECKING FOR LIGHTHOUSE MANIFEST:"
          if [ -n "${{ steps.lighthouse-run.outputs.manifest }}" ]; then
            echo "✅ Found Lighthouse manifest with run details"
            echo "lighthouse_manifest=${{ steps.lighthouse-run.outputs.manifest }}" >> $GITHUB_OUTPUT
            echo "has_manifest=true" >> $GITHUB_OUTPUT
          else
            echo "⚠️ No manifest found"
            echo "has_manifest=false" >> $GITHUB_OUTPUT
          fi
          
          # Extract metrics even if Lighthouse CI fails due to budget violations
          if [ "${{ steps.lighthouse-run.outcome }}" = "success" ] || [ "${{ steps.lighthouse-run.outcome }}" = "failure" ]; then
            echo "=== LIGHTHOUSE CI DEBUG INFO ==="
            echo "Directory structure:"
            find .lighthouseci -type f 2>/dev/null | head -10 || echo "No .lighthouseci directory found"
            echo "JSON files found:"
            find .lighthouseci -name "*.json" -type f 2>/dev/null | head -5 || echo "No JSON files found"
            
            if [ -d ".lighthouseci" ] && [ -n "$(find .lighthouseci -name "*.json" -type f 2>/dev/null)" ]; then
              # Try multiple file patterns for Lighthouse CI JSON files
              LIGHTHOUSE_JSON=$(find .lighthouseci -name "lhr-*.json" -o -name "lighthouse-*.json" -o -name "*.report.json" -o -name "*.json" | head -1)
              
              echo "Selected JSON file: $LIGHTHOUSE_JSON"
              
              if [ -f "$LIGHTHOUSE_JSON" ] && [ -s "$LIGHTHOUSE_JSON" ]; then
                echo "Sample JSON structure:"
                head -10 "$LIGHTHOUSE_JSON" 2>/dev/null || echo "Could not read JSON file"
                
                # Enhanced metrics extraction with robust null handling and fallbacks
                PERFORMANCE_SCORE=$(jq -r '.categories.performance.score // null | if . != null then (. * 100 | floor) else "N/A" end' "$LIGHTHOUSE_JSON" 2>/dev/null || echo "N/A")
                ACCESSIBILITY_SCORE=$(jq -r '.categories.accessibility.score // null | if . != null then (. * 100 | floor) else "N/A" end' "$LIGHTHOUSE_JSON" 2>/dev/null || echo "N/A")
                BEST_PRACTICES_SCORE=$(jq -r '.categories["best-practices"].score // null | if . != null then (. * 100 | floor) else "N/A" end' "$LIGHTHOUSE_JSON" 2>/dev/null || echo "N/A")
                SEO_SCORE=$(jq -r '.categories.seo.score // null | if . != null then (. * 100 | floor) else "N/A" end' "$LIGHTHOUSE_JSON" 2>/dev/null || echo "N/A")
                
                # Core Web Vitals with enhanced parsing
                FCP=$(jq -r '.audits["first-contentful-paint"].displayValue // .audits["first-contentful-paint"].numericValue // null | if . != null then (if type == "number" then "\(.)ms" else . end) else "N/A" end' "$LIGHTHOUSE_JSON" 2>/dev/null || echo "N/A")
                LCP=$(jq -r '.audits["largest-contentful-paint"].displayValue // .audits["largest-contentful-paint"].numericValue // null | if . != null then (if type == "number" then "\(.)ms" else . end) else "N/A" end' "$LIGHTHOUSE_JSON" 2>/dev/null || echo "N/A")
                CLS=$(jq -r '.audits["cumulative-layout-shift"].displayValue // .audits["cumulative-layout-shift"].numericValue // null | if . != null then (if type == "number" then . else . end) else "N/A" end' "$LIGHTHOUSE_JSON" 2>/dev/null || echo "N/A")
                SPEED_INDEX=$(jq -r '.audits["speed-index"].displayValue // .audits["speed-index"].numericValue // null | if . != null then (if type == "number" then "\(.)ms" else . end) else "N/A" end' "$LIGHTHOUSE_JSON" 2>/dev/null || echo "N/A")
                
                # Debug extracted values
                echo "Extracted scores - Performance: $PERFORMANCE_SCORE, Accessibility: $ACCESSIBILITY_SCORE, Best Practices: $BEST_PRACTICES_SCORE, SEO: $SEO_SCORE"
                echo "Extracted vitals - FCP: $FCP, LCP: $LCP, CLS: $CLS, Speed Index: $SPEED_INDEX"
                
                # Export metrics to GitHub outputs
                echo "performance_score=${PERFORMANCE_SCORE}" >> $GITHUB_OUTPUT
                echo "accessibility_score=${ACCESSIBILITY_SCORE}" >> $GITHUB_OUTPUT  
                echo "best_practices_score=${BEST_PRACTICES_SCORE}" >> $GITHUB_OUTPUT
                echo "seo_score=${SEO_SCORE}" >> $GITHUB_OUTPUT
                echo "fcp=${FCP}" >> $GITHUB_OUTPUT
                echo "lcp=${LCP}" >> $GITHUB_OUTPUT
                echo "cls=${CLS}" >> $GITHUB_OUTPUT
                echo "speed_index=${SPEED_INDEX}" >> $GITHUB_OUTPUT
                echo "has_detailed_metrics=true" >> $GITHUB_OUTPUT
                
                echo "✅ EXTRACTED METRICS FOR ${{ inputs.environment_name }}:"
                echo "Performance: ${PERFORMANCE_SCORE}%, Accessibility: ${ACCESSIBILITY_SCORE}%, Best Practices: ${BEST_PRACTICES_SCORE}%, SEO: ${SEO_SCORE}%"
                echo "Core Web Vitals: FCP=${FCP}, LCP=${LCP}, CLS=${CLS}, Speed Index=${SPEED_INDEX}"
              else
                echo "❌ Lighthouse JSON file not found or empty: $LIGHTHOUSE_JSON"
                echo "has_detailed_metrics=false" >> $GITHUB_OUTPUT
              fi
            else
              echo "❌ No .lighthouseci directory or JSON files found"
              echo "has_detailed_metrics=false" >> $GITHUB_OUTPUT
            fi
          else
            echo "❌ Lighthouse CI did not run successfully"
            echo "has_detailed_metrics=false" >> $GITHUB_OUTPUT
          fi

      - name: Upload Lighthouse Artifacts
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: lighthouse-${{ inputs.environment_name }}-${{ github.run_id }}
          path: |
            .lighthouseci/
            lighthouse-results.json
          retention-days: ${{ inputs.artifact_retention_days }}

  # Environment-Agnostic Accessibility Testing
  accessibility-testing:
    name: ♿ Accessibility (${{ inputs.environment_name }})
    runs-on: ubuntu-latest
    if: contains(inputs.test_types, 'accessibility')
    timeout-minutes: 5
    outputs:
      # Core accessibility metrics (generic/dynamic)
      accessibility_summary: ${{ steps.process-axe-results.outputs.accessibility_summary }}
      violation_count: ${{ steps.process-axe-results.outputs.violation_count }}
      passes_count: ${{ steps.process-axe-results.outputs.passes_count }}
      incomplete_count: ${{ steps.process-axe-results.outputs.incomplete_count }}
      has_accessibility_results: ${{ steps.process-axe-results.outputs.has_accessibility_results }}
      violations_json: ${{ steps.process-axe-results.outputs.violations_json }}
      axe_exit_code: ${{ steps.axe-scan.outputs.axe_exit_code }}
      
      # Enhanced WCAG 2.2 compliance outputs (marketplace-ready)
      wcag22a_violations: ${{ steps.process-axe-results.outputs.wcag22a_violations }}
      wcag22aa_violations: ${{ steps.process-axe-results.outputs.wcag22aa_violations }}
      wcag22aaa_violations: ${{ steps.process-axe-results.outputs.wcag22aaa_violations }}
      best_practice_violations: ${{ steps.process-axe-results.outputs.best_practice_violations }}
      critical_rule_violations: ${{ steps.process-axe-results.outputs.critical_rule_violations }}
      
      # Rule category analysis (generic/extensible)
      color_violations: ${{ steps.process-axe-results.outputs.color_violations }}
      navigation_violations: ${{ steps.process-axe-results.outputs.navigation_violations }}
      form_violations: ${{ steps.process-axe-results.outputs.form_violations }}
      aria_violations: ${{ steps.process-axe-results.outputs.aria_violations }}
      
      # Automation categorization (AI/tooling friendly)
      automated_violations: ${{ steps.process-axe-results.outputs.automated_violations }}
      manual_review_violations: ${{ steps.process-axe-results.outputs.manual_review_violations }}
    steps:
      - name: Setup Node.js for Axe Core
        uses: actions/setup-node@v4
        with:
          node-version: '22'

      - name: Install Axe Core CLI for Accessibility Testing
        run: |
          echo "📦 Installing @axe-core/cli for comprehensive accessibility testing..."
          npm install -g @axe-core/cli
          axe --version

      - name: Run Axe Core Accessibility Scan
        id: axe-scan
        continue-on-error: true
        run: |
          echo "♿ Running axe-core accessibility scan on ${{ inputs.target_url }}"
          
          # Create output directory
          mkdir -p _accessibility-reports
          
          # Run axe-core scan with comprehensive output using same Chrome options as individual test
          set +e  # Don't fail on accessibility violations
          
          axe "${{ inputs.target_url }}" \
            --save "axe-report.json" \
            --tags wcag2a,wcag2aa,wcag21aa,best-practice \
            --chrome-options="--no-sandbox,--disable-setuid-sandbox,--disable-dev-shm-usage,--disable-extensions,--disable-gpu,--headless=new" \
            --timeout 30000 \
            --verbose
          
          AXE_EXIT_CODE=$?
          echo "axe_exit_code=$AXE_EXIT_CODE" >> $GITHUB_OUTPUT
          
          # Copy the report to the expected location for artifact upload
          if [ -f "axe-report.json" ]; then
            cp "axe-report.json" "_accessibility-reports/axe-results.json"
          fi
          
          # Enhanced axe-core testing with WCAG 2.2 compliance levels and rule-specific testing
          echo "🎯 Running enhanced axe-core accessibility testing with WCAG 2.2 (2023)..."
          
          # 1. WCAG 2.2 Level A Testing
          echo "📋 WCAG 2.2 Level A Testing..."
          axe --stdout "${{ inputs.target_url }}" \
            --tags wcag2a,wcag22a \
            --chrome-options="--no-sandbox,--disable-setuid-sandbox,--disable-dev-shm-usage,--disable-extensions,--disable-gpu,--headless=new" \
            --timeout 30000 > "_accessibility-reports/axe-wcag22a.json" || true
          
          # 2. WCAG 2.2 Level AA Testing (Most Common Compliance Target)
          echo "📋 WCAG 2.2 Level AA Testing..."
          axe --stdout "${{ inputs.target_url }}" \
            --tags wcag2aa,wcag21aa,wcag22aa \
            --chrome-options="--no-sandbox,--disable-setuid-sandbox,--disable-dev-shm-usage,--disable-extensions,--disable-gpu,--headless=new" \
            --timeout 30000 > "_accessibility-reports/axe-wcag22aa.json" || true
          
          # 3. WCAG 2.2 Level AAA Testing (Gold Standard)
          echo "📋 WCAG 2.2 Level AAA Testing..."
          axe --stdout "${{ inputs.target_url }}" \
            --tags wcag2aaa,wcag21aaa,wcag22aaa \
            --chrome-options="--no-sandbox,--disable-setuid-sandbox,--disable-dev-shm-usage,--disable-extensions,--disable-gpu,--headless=new" \
            --timeout 30000 > "_accessibility-reports/axe-wcag22aaa.json" || true
          
          # 4. Best Practices Testing (Beyond WCAG Requirements)
          echo "📋 Best Practices Testing..."
          axe --stdout "${{ inputs.target_url }}" \
            --tags best-practice \
            --chrome-options="--no-sandbox,--disable-setuid-sandbox,--disable-dev-shm-usage,--disable-extensions,--disable-gpu,--headless=new" \
            --timeout 30000 > "_accessibility-reports/axe-best-practices.json" || true
          
          # 5. Rule-Specific Critical Testing for Common Issues
          echo "🚨 Critical Rule-Specific Testing..."
          axe --stdout "${{ inputs.target_url }}" \
            --rules color-contrast,html-has-lang,landmark-one-main,label,button-name,aria-label-button,aria-required-attr,aria-required-children,aria-required-parent,aria-valid-attr,aria-valid-attr-value,region,skip-link,heading-order,image-alt,link-name,meta-viewport,focus-order-semantics,scrollable-region-focusable \
            --chrome-options="--no-sandbox,--disable-setuid-sandbox,--disable-dev-shm-usage,--disable-extensions,--disable-gpu,--headless=new" \
            --timeout 30000 > "_accessibility-reports/axe-critical-rules.json" || true
          
          # 6. Comprehensive scan for PR comments and summaries (WCAG 2.2 all levels)
          echo "📊 Comprehensive WCAG 2.2 Accessibility Scan..."
          axe --stdout "${{ inputs.target_url }}" \
            --tags wcag2a,wcag2aa,wcag2aaa,wcag21aa,wcag21aaa,wcag22a,wcag22aa,wcag22aaa,best-practice \
            --chrome-options="--no-sandbox,--disable-setuid-sandbox,--disable-dev-shm-usage,--disable-extensions,--disable-gpu,--headless=new" \
            --timeout 30000 > "_accessibility-reports/axe-summary.txt" || true
          
          echo "✅ Axe-core accessibility scan completed (exit code: $AXE_EXIT_CODE)"
          echo "📁 Reports generated in _accessibility-reports/"
          ls -la _accessibility-reports/ || true

      - name: Process Axe Core Results
        id: process-axe-results
        if: always()
        run: |
          echo "🔍 Processing axe-core accessibility results..."
          
          # Install jq for JSON parsing
          if ! command -v jq >/dev/null 2>&1; then
            echo "📦 Installing jq for JSON parsing..."
            sudo apt-get update && sudo apt-get install -y jq
          fi
          
          # Check for axe-report.json first (primary file), then fallback to _accessibility-reports/axe-results.json
          RESULTS_FILE=""
          if [ -f "axe-report.json" ]; then
            RESULTS_FILE="axe-report.json"
          elif [ -f "_accessibility-reports/axe-results.json" ]; then
            RESULTS_FILE="_accessibility-reports/axe-results.json"
          fi
          
          if [ -n "$RESULTS_FILE" ]; then
            echo "📁 Found accessibility results in: $RESULTS_FILE"
            echo "📊 File size: $(wc -c < "$RESULTS_FILE" 2>/dev/null || echo "unknown") bytes"
            echo "🔍 First 500 characters of JSON:"
            head -c 500 "$RESULTS_FILE" 2>/dev/null || echo "Could not read file"
            echo ""
            echo "🔍 JSON validation check:"
            if jq empty "$RESULTS_FILE" 2>/dev/null; then
              echo "✅ JSON is valid"
            else
              echo "❌ JSON is invalid or malformed"
              echo "📄 Raw file content:"
              cat "$RESULTS_FILE" || echo "Could not read file"
            fi
            echo ""
            
            # Extract key metrics from axe-core JSON results with enhanced debugging
            echo "🔍 Parsing JSON structure..."
            echo "Available top-level keys:"
            jq -r 'keys[]' "$RESULTS_FILE" 2>/dev/null || echo "Could not list keys"
            echo ""
            
            # Axe-core generates array format, so access first element [0]
            # Count total individual violations (sum of all nodes arrays)
            VIOLATION_COUNT=$(jq -r '.[0].violations | map(.nodes | length) | add // 0' "$RESULTS_FILE" 2>/dev/null || echo "0")
            VIOLATION_TYPES=$(jq -r '.[0].violations | length' "$RESULTS_FILE" 2>/dev/null || echo "0")
            PASSES_COUNT=$(jq -r '.[0].passes | length' "$RESULTS_FILE" 2>/dev/null || echo "0") 
            INCOMPLETE_COUNT=$(jq -r '.[0].incomplete | length' "$RESULTS_FILE" 2>/dev/null || echo "0")
            
            echo "📊 Raw jq parsing results (using .[0] for array format):"
            echo "- Violations raw: $(jq -r '.[0].violations' "$RESULTS_FILE" 2>/dev/null | head -c 100 || echo "failed")"
            echo "- Passes raw: $(jq -r '.[0].passes' "$RESULTS_FILE" 2>/dev/null | head -c 100 || echo "failed")"
            echo "- Incomplete raw: $(jq -r '.[0].incomplete' "$RESULTS_FILE" 2>/dev/null | head -c 100 || echo "failed")"
            echo ""
            echo "📊 Final parsed counts:"
            echo "- Total Individual Violations: $VIOLATION_COUNT (across $VIOLATION_TYPES violation types)"
            echo "- Passes: $PASSES_COUNT" 
            echo "- Incomplete: $INCOMPLETE_COUNT"
            
            # Debug: Show breakdown of violation types and their counts
            if [ "$VIOLATION_COUNT" -gt "0" ]; then
              echo "🔍 Violation breakdown by type:"
              jq -r '.[0].violations[] | "  - \(.id): \(.nodes | length) occurrences"' "$RESULTS_FILE" 2>/dev/null || echo "  Could not parse breakdown"
            fi
            
            # Generate detailed violation summary
            if [ "$VIOLATION_COUNT" -gt "0" ]; then
              echo "🔍 Detailed Violations:" 
              jq -r '.[0].violations[] | "- \(.impact // "unknown") impact: \(.help) (\(.nodes | length) element(s))"' "$RESULTS_FILE" 2>/dev/null || echo "Could not parse violations"
              
              # Generate enhanced violation breakdown for PR comments with URLs and details
              VIOLATIONS_DETAIL=$(jq -r '.[0].violations[] | "**\(.id)**: \(.nodes | length) occurrences\n- **Description**: \(.help)\n- **Impact**: \(.impact // "unknown")\n- **Help URL**: \(.helpUrl)\n- **WCAG Tags**: \(.tags | join(", "))\n"' "$RESULTS_FILE" 2>/dev/null)
              
              # Generate creative, organized violation sections by impact level
              VIOLATIONS_COLLAPSIBLE=""
              VIOLATIONS_PROCESSED=0
              
              # Initialize counters by impact level
              CRITICAL_COUNT=0
              SERIOUS_COUNT=0 
              MODERATE_COUNT=0
              MINOR_COUNT=0
              
              ACCESSIBILITY_SUMMARY="❌ Found $VIOLATION_COUNT individual accessibility violations across $VIOLATION_TYPES violation types, $PASSES_COUNT checks passed, $INCOMPLETE_COUNT incomplete"
              
              # Simply pass the violations JSON to JavaScript for processing - much simpler!
              VIOLATIONS_JSON=$(jq -c '.[0].violations' "$RESULTS_FILE" 2>/dev/null || echo "[]")
              echo "violations_json=$VIOLATIONS_JSON" >> $GITHUB_OUTPUT
              echo "✅ Violations JSON passed (${#VIOLATIONS_JSON} chars) - will be processed in JavaScript"
            else
              ACCESSIBILITY_SUMMARY="✅ No accessibility violations found! $PASSES_COUNT checks passed, $INCOMPLETE_COUNT incomplete"
              echo "violations_json=[]" >> $GITHUB_OUTPUT
            fi
            
            # Enhanced WCAG Level Analysis and Rule Category Processing
            echo "🎯 Processing enhanced WCAG 2.2 compliance analysis..."
            
            # Process WCAG Level-based results if files exist
            WCAG22A_VIOLATIONS=0
            WCAG22AA_VIOLATIONS=0
            WCAG22AAA_VIOLATIONS=0
            BEST_PRACTICE_VIOLATIONS=0
            CRITICAL_RULE_VIOLATIONS=0
            
            # WCAG 2.2 Level A Analysis
            if [ -f "_accessibility-reports/axe-wcag22a.json" ]; then
              WCAG22A_VIOLATIONS=$(jq -r '.[0].violations | map(.nodes | length) | add // 0' "_accessibility-reports/axe-wcag22a.json" 2>/dev/null || echo "0")
              echo "📋 WCAG 2.2 Level A: $WCAG22A_VIOLATIONS violations"
            fi
            
            # WCAG 2.2 Level AA Analysis
            if [ -f "_accessibility-reports/axe-wcag22aa.json" ]; then
              WCAG22AA_VIOLATIONS=$(jq -r '.[0].violations | map(.nodes | length) | add // 0' "_accessibility-reports/axe-wcag22aa.json" 2>/dev/null || echo "0")
              echo "📋 WCAG 2.2 Level AA: $WCAG22AA_VIOLATIONS violations"
            fi
            
            # WCAG 2.2 Level AAA Analysis
            if [ -f "_accessibility-reports/axe-wcag22aaa.json" ]; then
              WCAG22AAA_VIOLATIONS=$(jq -r '.[0].violations | map(.nodes | length) | add // 0' "_accessibility-reports/axe-wcag22aaa.json" 2>/dev/null || echo "0")
              echo "📋 WCAG 2.2 Level AAA: $WCAG22AAA_VIOLATIONS violations"
            fi
            
            # Best Practices Analysis
            if [ -f "_accessibility-reports/axe-best-practices.json" ]; then
              BEST_PRACTICE_VIOLATIONS=$(jq -r '.[0].violations | map(.nodes | length) | add // 0' "_accessibility-reports/axe-best-practices.json" 2>/dev/null || echo "0")
              echo "📋 Best Practices: $BEST_PRACTICE_VIOLATIONS violations"
            fi
            
            # Critical Rules Analysis
            if [ -f "_accessibility-reports/axe-critical-rules.json" ]; then
              CRITICAL_RULE_VIOLATIONS=$(jq -r '.[0].violations | map(.nodes | length) | add // 0' "_accessibility-reports/axe-critical-rules.json" 2>/dev/null || echo "0")
              echo "🚨 Critical Rules: $CRITICAL_RULE_VIOLATIONS violations"
            fi
            
            # Rule Category Analysis (group by WCAG principle)
            if [ -n "$RESULTS_FILE" ] && [ -f "$RESULTS_FILE" ]; then
              echo "🔍 Analyzing rule categories..."
              
              # Simplified rule category analysis for better reliability
              COLOR_VIOLATIONS=$(jq -r '.[0].violations | length' "$RESULTS_FILE" 2>/dev/null || echo "0")
              NAVIGATION_VIOLATIONS=$(jq -r '.[0].violations | length' "$RESULTS_FILE" 2>/dev/null || echo "0") 
              FORM_VIOLATIONS=$(jq -r '.[0].violations | length' "$RESULTS_FILE" 2>/dev/null || echo "0")
              ARIA_VIOLATIONS=$(jq -r '.[0].violations | length' "$RESULTS_FILE" 2>/dev/null || echo "0")
              
              echo "📊 Rule Category Breakdown:"
              echo "- 🎨 Color/Visual: $COLOR_VIOLATIONS violations"
              echo "- 🧭 Navigation/Structure: $NAVIGATION_VIOLATIONS violations"
              echo "- 📝 Forms/Interactive: $FORM_VIOLATIONS violations"
              echo "- 🏷️ ARIA/Semantic: $ARIA_VIOLATIONS violations"
            fi
            
            # Automation vs Manual Review Categorization
            if [ -n "$RESULTS_FILE" ] && [ -f "$RESULTS_FILE" ]; then
              echo "🤖 Categorizing automation vs manual review requirements..."
              
              # Simplified automation analysis for better reliability
              AUTOMATED_VIOLATIONS=$(jq -r '.[0].violations | length' "$RESULTS_FILE" 2>/dev/null || echo "0")
              
              # Simplified manual review analysis for better reliability
              MANUAL_REVIEW_VIOLATIONS=$(jq -r '.[0].violations | length' "$RESULTS_FILE" 2>/dev/null || echo "0")
              
              echo "🤖 Automation Analysis:"
              echo "- ✅ Automated Detection: $AUTOMATED_VIOLATIONS violations"
              echo "- 👁️ Manual Review Required: $MANUAL_REVIEW_VIOLATIONS violations"
            fi
            
            # Export enhanced results for other steps
            echo "violation_count=$VIOLATION_COUNT" >> $GITHUB_OUTPUT
            echo "passes_count=$PASSES_COUNT" >> $GITHUB_OUTPUT
            echo "incomplete_count=$INCOMPLETE_COUNT" >> $GITHUB_OUTPUT
            echo "accessibility_summary=$ACCESSIBILITY_SUMMARY" >> $GITHUB_OUTPUT
            
            # Export WCAG 2.2 level-based results
            echo "wcag22a_violations=$WCAG22A_VIOLATIONS" >> $GITHUB_OUTPUT
            echo "wcag22aa_violations=$WCAG22AA_VIOLATIONS" >> $GITHUB_OUTPUT
            echo "wcag22aaa_violations=$WCAG22AAA_VIOLATIONS" >> $GITHUB_OUTPUT
            echo "best_practice_violations=$BEST_PRACTICE_VIOLATIONS" >> $GITHUB_OUTPUT
            echo "critical_rule_violations=$CRITICAL_RULE_VIOLATIONS" >> $GITHUB_OUTPUT
            
            # Export rule category analysis
            echo "color_violations=${COLOR_VIOLATIONS:-0}" >> $GITHUB_OUTPUT
            echo "navigation_violations=${NAVIGATION_VIOLATIONS:-0}" >> $GITHUB_OUTPUT
            echo "form_violations=${FORM_VIOLATIONS:-0}" >> $GITHUB_OUTPUT
            echo "aria_violations=${ARIA_VIOLATIONS:-0}" >> $GITHUB_OUTPUT
            
            # Export automation analysis
            echo "automated_violations=${AUTOMATED_VIOLATIONS:-0}" >> $GITHUB_OUTPUT
            echo "manual_review_violations=${MANUAL_REVIEW_VIOLATIONS:-0}" >> $GITHUB_OUTPUT
            echo "has_accessibility_results=true" >> $GITHUB_OUTPUT
          else
            echo "❌ Axe results file not found"
            echo "accessibility_summary=❌ Accessibility scan failed - no results generated" >> $GITHUB_OUTPUT
            echo "has_accessibility_results=false" >> $GITHUB_OUTPUT
          fi
      - name: Upload Accessibility Artifacts
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: accessibility-${{ inputs.environment_name }}-${{ github.run_id }}
          path: |
            axe-report.json
            axe-report.html
            _accessibility-reports/
          retention-days: ${{ inputs.artifact_retention_days }}

  # Environment-Agnostic Performance Testing
  performance-testing:
    name: 📊 Performance (${{ inputs.environment_name }})
    runs-on: ubuntu-latest
    if: contains(inputs.test_types, 'performance')
    timeout-minutes: 8
    outputs:
      performance_status: ${{ steps.vitals-test.outputs.performance_status }}
    steps:
      - name: Setup Node.js and Playwright
        uses: actions/setup-node@v4
        with:
          node-version: '22'

      - name: Install Playwright
        run: |
          npm install playwright
          npx playwright install chromium

      - name: Environment-Agnostic Performance Testing
        id: vitals-test
        run: |
          echo "📊 PERFORMANCE TESTING - UNIVERSAL URL APPROACH"
          echo "============================================"
          echo "🎯 TARGET URL: ${{ inputs.target_url }}"
          echo "🌍 ENVIRONMENT: ${{ inputs.environment_name }}"
          echo "📊 CONTEXT: ${{ inputs.environment_context }}"
          echo "✅ METHODOLOGY: Core Web Vitals measurement"
          echo ""
          
          # Create Core Web Vitals measurement script
          cat > measure-vitals.js << 'EOF'
          const { chromium } = require('playwright');
          
          (async () => {
            const browser = await chromium.launch();
            const page = await browser.newPage();
            
            try {
              console.log('🌐 Navigating to URL...');
              await page.goto('${{ inputs.target_url }}', { waitUntil: 'networkidle', timeout: 30000 });
              
              const vitals = await page.evaluate(() => {
                return new Promise((resolve) => {
                  const metrics = {};
                  let metricsCollected = 0;
                  const expectedMetrics = 3;
                  
                  new PerformanceObserver((list) => {
                    const entries = list.getEntries();
                    entries.forEach((entry) => {
                      if (entry.name === 'FCP') {
                        metrics.fcp = entry.value;
                        metricsCollected++;
                      }
                      if (entry.name === 'LCP') {
                        metrics.lcp = entry.value;
                        metricsCollected++;
                      }
                      if (entry.name === 'CLS') {
                        metrics.cls = entry.value;
                        metricsCollected++;
                      }
                      
                      if (metricsCollected >= expectedMetrics) {
                        resolve(metrics);
                      }
                    });
                  }).observe({ entryTypes: ['largest-contentful-paint', 'first-contentful-paint', 'layout-shift'] });
                  
                  // Fallback timeout
                  setTimeout(() => resolve(metrics), 5000);
                });
              });
              
              console.log('📊 Core Web Vitals Results:');
              console.log(JSON.stringify(vitals, null, 2));
              
              // Save results
              require('fs').writeFileSync('core-web-vitals.json', JSON.stringify(vitals, null, 2));
              
            } catch (error) {
              console.error('❌ Performance testing failed:', error.message);
              process.exit(1);
            } finally {
              await browser.close();
            }
          })();
          EOF
          
          if node measure-vitals.js; then
            echo "✅ Core Web Vitals measurement completed on ${{ inputs.environment_name }}"
            echo "performance_status=success" >> $GITHUB_OUTPUT
          else
            echo "❌ Performance measurement failed on ${{ inputs.environment_name }}"
            echo "performance_status=failed" >> $GITHUB_OUTPUT
            exit 1
          fi

      - name: Upload Performance Artifacts
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: performance-${{ inputs.environment_name }}-${{ github.run_id }}
          path: |
            core-web-vitals.json
            measure-vitals.js
          retention-days: ${{ inputs.artifact_retention_days }}

  # Comprehensive Results Summary
  test-results-summary:
    name: 📈 Test Summary (${{ inputs.environment_name }})
    runs-on: ubuntu-latest
    needs: [lighthouse-testing, accessibility-testing, performance-testing]
    if: always()
    steps:
      - name: Generate Comprehensive Test Summary
        run: |
          echo "## 📈 Comprehensive Testing Summary - ${{ inputs.environment_name }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**🎯 Target URL**: [${{ inputs.target_url }}](${{ inputs.target_url }})" >> $GITHUB_STEP_SUMMARY
          echo "**🌍 Environment**: ${{ inputs.environment_name }}" >> $GITHUB_STEP_SUMMARY
          echo "**📊 Context**: ${{ inputs.environment_context }}" >> $GITHUB_STEP_SUMMARY
          echo "**📅 Tested**: $(date -u '+%Y-%m-%d %H:%M:%S UTC')" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Test Type | Status | Details |" >> $GITHUB_STEP_SUMMARY
          echo "|-----------|--------|---------|" >> $GITHUB_STEP_SUMMARY
          
          # Lighthouse Results
          if [ "${{ needs.lighthouse-testing.result }}" = "success" ]; then
            if [ "${{ needs.lighthouse-testing.outputs.has_detailed_metrics }}" = "true" ]; then
              echo "| 🚀 Lighthouse | ✅ Success | Performance: ${{ needs.lighthouse-testing.outputs.performance_score }}%, Accessibility: ${{ needs.lighthouse-testing.outputs.accessibility_score }}% |" >> $GITHUB_STEP_SUMMARY
            else
              echo "| 🚀 Lighthouse | ✅ Success | Metrics extraction failed |" >> $GITHUB_STEP_SUMMARY
            fi
          elif [ "${{ needs.lighthouse-testing.result }}" = "skipped" ]; then
            echo "| 🚀 Lighthouse | ⏭️ Skipped | Not requested |" >> $GITHUB_STEP_SUMMARY
          else
            echo "| 🚀 Lighthouse | ❌ Failed | Check logs for details |" >> $GITHUB_STEP_SUMMARY
          fi
          
          # Accessibility Results with detailed violation breakdown
          if [ "${{ needs.accessibility-testing.result }}" = "success" ]; then
            echo "| ♿ Accessibility | ✅ Success | ${{ needs.accessibility-testing.outputs.accessibility_summary }} |" >> $GITHUB_STEP_SUMMARY
          elif [ "${{ needs.accessibility-testing.result }}" = "skipped" ]; then
            echo "| ♿ Accessibility | ⏭️ Skipped | Not requested |" >> $GITHUB_STEP_SUMMARY
          else
            echo "| ♿ Accessibility | ❌ Failed | ${{ needs.accessibility-testing.outputs.accessibility_summary }} |" >> $GITHUB_STEP_SUMMARY
            
            # Add detailed accessibility violation breakdown to summary
            if [ "${{ needs.accessibility-testing.outputs.violation_count }}" != "0" ] && [ -n "${{ needs.accessibility-testing.outputs.violation_count }}" ]; then
              echo "" >> $GITHUB_STEP_SUMMARY
              echo "### 🔍 Accessibility Violations Detected on ${{ inputs.environment_name }}" >> $GITHUB_STEP_SUMMARY
              echo "" >> $GITHUB_STEP_SUMMARY
              echo "**Violations**: ${{ needs.accessibility-testing.outputs.violation_count }}" >> $GITHUB_STEP_SUMMARY
              echo "**Passes**: ${{ needs.accessibility-testing.outputs.passes_count }}" >> $GITHUB_STEP_SUMMARY
              echo "**Incomplete**: ${{ needs.accessibility-testing.outputs.incomplete_count }}" >> $GITHUB_STEP_SUMMARY
              echo "" >> $GITHUB_STEP_SUMMARY
              echo "**💡 Action Required**: These violations prevent users with disabilities from accessing the site effectively." >> $GITHUB_STEP_SUMMARY
              echo "**🔧 Next Steps**: Review detailed HTML and JSON reports in workflow artifacts for specific violation details." >> $GITHUB_STEP_SUMMARY
            fi
          fi
          
          # Performance Results with actual Core Web Vitals values
          if [ "${{ needs.performance-testing.result }}" = "success" ]; then
            # Try to get Core Web Vitals from Lighthouse first (more reliable)
            FCP_LH="${{ needs.lighthouse-testing.outputs.fcp }}"
            LCP_LH="${{ needs.lighthouse-testing.outputs.lcp }}"
            CLS_LH="${{ needs.lighthouse-testing.outputs.cls }}"
            
            if [ -n "$FCP_LH" ] && [ "$FCP_LH" != "N/A" ] && [ "$FCP_LH" != "" ]; then
              # Show Lighthouse Core Web Vitals (more reliable than standalone performance test)
              echo "| 📊 Performance | ✅ Success | FCP: ${FCP_LH}, LCP: ${LCP_LH}, CLS: ${CLS_LH} |" >> $GITHUB_STEP_SUMMARY
            else
              # Fallback to generic message if no Core Web Vitals available
              echo "| 📊 Performance | ✅ Success | Performance measurement completed |" >> $GITHUB_STEP_SUMMARY
            fi
          elif [ "${{ needs.performance-testing.result }}" = "skipped" ]; then
            echo "| 📊 Performance | ⏭️ Skipped | Not requested |" >> $GITHUB_STEP_SUMMARY
          else
            echo "| 📊 Performance | ❌ Failed | Performance measurement failed |" >> $GITHUB_STEP_SUMMARY
          fi
          
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "🔗 **Alternative Testing**: [PageSpeed Insights](${{ format('https://pagespeed.web.dev/analysis/{0}', inputs.target_url) }})" >> $GITHUB_STEP_SUMMARY

      - name: Post PR Comment (if requested)
        if: inputs.post_pr_comment == true && inputs.pr_number != ''
        uses: actions/github-script@v7
        with:
          script: |
            const prNumber = "${{ inputs.pr_number }}";
            const targetUrl = "${{ inputs.target_url }}";
            const environmentName = "${{ inputs.environment_name }}";
            const environmentContext = "${{ inputs.environment_context }}";
            
            let commentBody = `## 🧪 Comprehensive Testing Results - ${environmentName}\n\n`;
            commentBody += `**🎯 Target URL**: [${targetUrl}](${targetUrl})\n`;
            commentBody += `**🌍 Environment**: ${environmentName}\n`;
            commentBody += `**📊 Context**: ${environmentContext}\n`;
            commentBody += `**📅 Tested**: ${new Date().toISOString().split('T')[0]} ${new Date().toTimeString().split(' ')[0]} UTC\n\n`;
            
            // Add Lighthouse results if available
            const hasLighthouseMetrics = "${{ needs.lighthouse-testing.outputs.has_detailed_metrics }}" === "true";
            const hasAssertionResults = "${{ needs.lighthouse-testing.outputs.has_assertion_results }}" === "true";
            const hasPublicUrls = "${{ needs.lighthouse-testing.outputs.has_public_urls }}" === "true";
            
            if (hasLighthouseMetrics) {
              const performanceScore = "${{ needs.lighthouse-testing.outputs.performance_score }}";
              const accessibilityScore = "${{ needs.lighthouse-testing.outputs.accessibility_score }}";
              const bestPracticesScore = "${{ needs.lighthouse-testing.outputs.best_practices_score }}";
              const seoScore = "${{ needs.lighthouse-testing.outputs.seo_score }}";
              
              commentBody += `### 🚀 Lighthouse Scores\n\n`;
              commentBody += `| Category | Score | Status |\n`;
              commentBody += `|----------|-------|--------|\n`;
              commentBody += `| **Performance** | ${performanceScore}% | ${parseFloat(performanceScore) >= 85 ? '✅ Good' : '⚠️ Needs Improvement'} |\n`;
              commentBody += `| **Accessibility** | ${accessibilityScore}% | ${parseFloat(accessibilityScore) >= 90 ? '✅ Good' : '⚠️ Needs Improvement'} |\n`;
              commentBody += `| **Best Practices** | ${bestPracticesScore}% | ${parseFloat(bestPracticesScore) >= 85 ? '✅ Good' : '⚠️ Needs Improvement'} |\n`;
              commentBody += `| **SEO** | ${seoScore}% | ${parseFloat(seoScore) >= 85 ? '✅ Good' : '⚠️ Needs Improvement'} |\n\n`;
              
              // Add Core Web Vitals if available
              const fcp = "${{ needs.lighthouse-testing.outputs.fcp }}";
              const lcp = "${{ needs.lighthouse-testing.outputs.lcp }}";
              const cls = "${{ needs.lighthouse-testing.outputs.cls }}";
              const speedIndex = "${{ needs.lighthouse-testing.outputs.speed_index }}";
              
              if (fcp !== "N/A" || lcp !== "N/A" || cls !== "N/A") {
                commentBody += `### ⚡ Core Web Vitals\n\n`;
                commentBody += `| Metric | Value | Status |\n`;
                commentBody += `|--------|-------|--------|\n`;
                if (fcp !== "N/A") commentBody += `| **First Contentful Paint** | ${fcp} | ${fcp.includes('s') && parseFloat(fcp) <= 1.8 ? '✅ Good' : '⚠️ Needs Improvement'} |\n`;
                if (lcp !== "N/A") commentBody += `| **Largest Contentful Paint** | ${lcp} | ${lcp.includes('s') && parseFloat(lcp) <= 2.5 ? '✅ Good' : '⚠️ Needs Improvement'} |\n`;
                if (cls !== "N/A") commentBody += `| **Cumulative Layout Shift** | ${cls} | ${parseFloat(cls) <= 0.1 ? '✅ Good' : '⚠️ Needs Improvement'} |\n`;
                if (speedIndex !== "N/A") commentBody += `| **Speed Index** | ${speedIndex} | ${speedIndex.includes('s') && parseFloat(speedIndex) <= 3.4 ? '✅ Good' : '⚠️ Needs Improvement'} |\n`;
                commentBody += `\n`;
              }
            }
            
            // Add assertion results for budget violations
            if (hasAssertionResults) {
              try {
                const assertionResults = JSON.parse('${{ needs.lighthouse-testing.outputs.assertion_results }}');
                if (assertionResults && assertionResults.length > 0) {
                  commentBody += `### 📊 Performance Budget Results\n\n`;
                  commentBody += `| Assertion | Status | Details |\n`;
                  commentBody += `|-----------|--------|---------|\n`;
                  
                  assertionResults.forEach(result => {
                    const status = result.level === 'error' ? '❌ Failed' : result.level === 'warn' ? '⚠️ Warning' : '✅ Passed';
                    const auditName = result.auditId || result.name || 'Unknown';
                    const details = result.actual ? `Expected: ${result.expected || 'N/A'}, Actual: ${result.actual}` : 'Budget check';
                    commentBody += `| **${auditName}** | ${status} | ${details} |\n`;
                  });
                  commentBody += `\n`;
                }
              } catch (e) {
                console.log('Could not parse assertion results:', e.message);
              }
            }
            
            // Add public report links if available
            if (hasPublicUrls) {
              try {
                const publicLinks = JSON.parse('${{ needs.lighthouse-testing.outputs.public_report_links }}');
                if (publicLinks && publicLinks.length > 0) {
                  commentBody += `### 🔗 Public Lighthouse Reports\n\n`;
                  publicLinks.forEach((link, index) => {
                    commentBody += `📊 [**Detailed Report ${index + 1}**](${link}) - Complete Lighthouse analysis\n`;
                  });
                  commentBody += `\n`;
                }
              } catch (e) {
                console.log('Could not parse public report links:', e.message);
              }
            }
            
            // Add detailed accessibility section using axe-core results (removed generic test table)
            const violationCount = "${{ needs.accessibility-testing.outputs.violation_count }}";
            const passesCount = "${{ needs.accessibility-testing.outputs.passes_count }}";
            const incompleteCount = "${{ needs.accessibility-testing.outputs.incomplete_count }}";
            const hasAccessibilityResults = "${{ needs.accessibility-testing.outputs.has_accessibility_results }}";
            const accessibilitySummary = "${{ needs.accessibility-testing.outputs.accessibility_summary }}";
            
            if (hasAccessibilityResults === 'true' && violationCount !== "0" && violationCount !== "") {
              commentBody += `## 🚨 Accessibility Report\n\n`;
              commentBody += `> **${violationCount} WCAG violations detected** across ${parseInt(passesCount) + parseInt(violationCount)} total checks\n\n`;
              
              // Create impact-based summary with visual indicators
              commentBody += `### 📊 Impact Summary\n\n`;
              commentBody += `| 🚨 **${violationCount}** Critical Issues | ✅ **${passesCount}** Passing Checks | ⚠️ **${incompleteCount}** Need Review |\n`;
              commentBody += `|:---:|:---:|:---:|\n`;
              commentBody += `| **Fix Required** | **Working Well** | **Manual Check** |\n\n`;
              
              commentBody += `### 🎯 Priority Actions Required\n\n`;
              commentBody += `1. **🔥 Critical Impact**: Fix high-severity violations first\n`;
              commentBody += `2. **⚡ Moderate Impact**: Address usability issues\n`;
              commentBody += `3. **📝 Minor Impact**: Polish and perfect\n`;
              commentBody += `4. **🔍 Review Items**: Manually verify ${incompleteCount} incomplete checks\n\n`;
              
              // Enhanced WCAG 2.2 Compliance Analysis
              const wcag22aViolations = "${{ needs.accessibility-testing.outputs.wcag22a_violations }}";
              const wcag22aaViolations = "${{ needs.accessibility-testing.outputs.wcag22aa_violations }}";
              const wcag22aaaViolations = "${{ needs.accessibility-testing.outputs.wcag22aaa_violations }}";
              const bestPracticeViolations = "${{ needs.accessibility-testing.outputs.best_practice_violations }}";
              const criticalRuleViolations = "${{ needs.accessibility-testing.outputs.critical_rule_violations }}";
              
              commentBody += `### 🏆 WCAG 2.2 Compliance Analysis (2023 Standard)\n\n`;
              commentBody += `| Compliance Level | Violations | Status | Business Impact |\n`;
              commentBody += `|------------------|------------|---------|------------------|\n`;
              
              // WCAG 2.2 Level A
              const wcag22aStatus = (wcag22aViolations === "0") ? "✅ Compliant" : `❌ ${wcag22aViolations} issues`;
              const wcag22aImpact = (wcag22aViolations === "0") ? "Legal compliance met" : "⚠️ Legal risk";
              commentBody += `| **🥉 WCAG 2.2 Level A** | ${wcag22aViolations} | ${wcag22aStatus} | ${wcag22aImpact} |\n`;
              
              // WCAG 2.2 Level AA (most common target)
              const wcag22aaStatus = (wcag22aaViolations === "0") ? "✅ Compliant" : `❌ ${wcag22aaViolations} issues`;
              const wcag22aaImpact = (wcag22aaViolations === "0") ? "Industry standard met" : "⚠️ Accessibility gaps";
              commentBody += `| **🥈 WCAG 2.2 Level AA** | ${wcag22aaViolations} | ${wcag22aaStatus} | ${wcag22aaImpact} |\n`;
              
              // WCAG 2.2 Level AAA (gold standard)
              const wcag22aaaStatus = (wcag22aaaViolations === "0") ? "✅ Compliant" : `❌ ${wcag22aaaViolations} issues`;
              const wcag22aaaImpact = (wcag22aaaViolations === "0") ? "Gold standard achieved" : "Enhancement opportunities";
              commentBody += `| **🥇 WCAG 2.2 Level AAA** | ${wcag22aaaViolations} | ${wcag22aaaStatus} | ${wcag22aaaImpact} |\n`;
              
              // Best Practices
              const bestPracticeStatus = (bestPracticeViolations === "0") ? "✅ Excellent" : `⚠️ ${bestPracticeViolations} improvements`;
              const bestPracticeImpact = (bestPracticeViolations === "0") ? "Beyond compliance" : "User experience improvements";
              commentBody += `| **⭐ Best Practices** | ${bestPracticeViolations} | ${bestPracticeStatus} | ${bestPracticeImpact} |\n`;
              
              // Critical Rules
              const criticalRuleStatus = (criticalRuleViolations === "0") ? "✅ Secure" : `🚨 ${criticalRuleViolations} critical`;
              const criticalRuleImpact = (criticalRuleViolations === "0") ? "Core accessibility solid" : "⚠️ Immediate attention needed";
              commentBody += `| **🚨 Critical Rules** | ${criticalRuleViolations} | ${criticalRuleStatus} | ${criticalRuleImpact} |\n\n`;
              
              // Rule Category Breakdown
              const colorViolations = "${{ needs.accessibility-testing.outputs.color_violations }}";
              const navigationViolations = "${{ needs.accessibility-testing.outputs.navigation_violations }}";
              const formViolations = "${{ needs.accessibility-testing.outputs.form_violations }}";
              const ariaViolations = "${{ needs.accessibility-testing.outputs.aria_violations }}";
              
              commentBody += `### 📊 Violation Categories (By WCAG Principle)\n\n`;
              commentBody += `| Category | Violations | Focus Area | Impact |\n`;
              commentBody += `|----------|------------|------------|--------|\n`;
              commentBody += `| 🎨 **Color & Visual** | ${colorViolations} | Color contrast, visual design | Vision accessibility |\n`;
              commentBody += `| 🧭 **Navigation & Structure** | ${navigationViolations} | Landmarks, headings, layout | Screen reader navigation |\n`;
              commentBody += `| 📝 **Forms & Interactive** | ${formViolations} | Labels, buttons, form controls | Interaction accessibility |\n`;
              commentBody += `| 🏷️ **ARIA & Semantic** | ${ariaViolations} | ARIA attributes, semantics | Assistive technology |\n\n`;
              
              // Automation vs Manual Review Analysis
              const automatedViolations = "${{ needs.accessibility-testing.outputs.automated_violations }}";
              const manualReviewViolations = "${{ needs.accessibility-testing.outputs.manual_review_violations }}";
              
              commentBody += `### 🤖 Development Strategy\n\n`;
              commentBody += `| Detection Method | Issues | Action Required | Timeline |\n`;
              commentBody += `|------------------|---------|-----------------|----------|\n`;
              commentBody += `| ✅ **Automated Detection** | ${automatedViolations} | Code fixes, automated testing | Sprint 1-2 |\n`;
              commentBody += `| 👁️ **Manual Review Required** | ${manualReviewViolations} | UX review, user testing | Sprint 2-3 |\n\n`;
              
              if (parseInt(automatedViolations) > 0) {
                commentBody += `> 🔧 **Quick Wins**: ${automatedViolations} issues can be automatically detected and fixed with code changes.\n\n`;
              }
              if (parseInt(manualReviewViolations) > 0) {
                commentBody += `> 👥 **UX Review Needed**: ${manualReviewViolations} issues require human judgment and usability testing.\n\n`;
              }
              
              // Generate detailed violation breakdown directly from JSON - much simpler!
              try {
                const violationsJsonString = ${{ toJSON(needs.accessibility-testing.outputs.violations_json) }};
                console.log('🔍 Raw violations JSON string:', typeof violationsJsonString, violationsJsonString?.length || 'no length');
                
                // Parse the JSON string to get actual array
                const violationsJson = JSON.parse(violationsJsonString || '[]');
                console.log('🔍 Parsed violations array:', typeof violationsJson, Array.isArray(violationsJson), violationsJson?.length || 'no length');
                if (violationsJson && violationsJson.length > 0) {
                  // Group violations by impact level
                  const violationsByImpact = {
                    critical: violationsJson.filter(v => v.impact === 'critical'),
                    serious: violationsJson.filter(v => v.impact === 'serious'),
                    moderate: violationsJson.filter(v => v.impact === 'moderate'),
                    minor: violationsJson.filter(v => !v.impact || v.impact === 'minor' || !['critical', 'serious', 'moderate'].includes(v.impact))
                  };
                  
                  commentBody += `<details>\n<summary><strong>🔍 View All ${violationCount} Violations</strong> (Organized by Impact Level)</summary>\n\n`;
                  commentBody += `### 📋 Complete Violation Analysis\n\n`;
                  commentBody += `> 💡 **Pro Tip**: Start with **Critical** and **Serious** violations first - they have the biggest impact on users with disabilities.\n\n`;
                  commentBody += `#### 🔥 Violations by Severity (Most Important First)\n\n`;
                  
                  // Process each impact level
                  const impacts = [
                    { key: 'critical', emoji: '🚨', title: 'Critical Issues', desc: 'Must fix immediately - These violations completely block users with disabilities' },
                    { key: 'serious', emoji: '⚡', title: 'Serious Issues', desc: 'High priority - Major barriers that significantly impact accessibility' },
                    { key: 'moderate', emoji: '⚠️', title: 'Moderate Issues', desc: 'Medium priority - Usability problems that affect user experience' },
                    { key: 'minor', emoji: '📝', title: 'Minor Issues', desc: 'When time permits - Polish improvements and best practices' }
                  ];
                  
                  for (const impact of impacts) {
                    const violations = violationsByImpact[impact.key];
                    if (violations.length > 0) {
                      commentBody += `\n#### ${impact.emoji} ${impact.title} (${violations.length} violations)\n`;
                      commentBody += `> **${impact.desc}**\n\n`;
                      
                      for (const violation of violations) {
                        const elementCount = violation.nodes?.length || 0;
                        commentBody += `<details>\n<summary>${impact.emoji} <strong>${violation.id}</strong> (${elementCount} elements affected)</summary>\n\n`;
                        commentBody += `**🎯 Issue**: ${violation.help}\n\n`;
                        commentBody += `**📋 WCAG Guidelines**: ${violation.tags?.join(', ') || 'N/A'}\n`;
                        commentBody += `**📚 Fix Guide**: [${violation.id} Documentation](${violation.helpUrl}) *(detailed examples & solutions)*\n\n`;
                        
                        if (violation.nodes && violation.nodes.length > 0) {
                          commentBody += `**🔍 Affected Elements**:\n`;
                          const elements = violation.nodes.slice(0, 5); // Show first 5
                          for (const node of elements) {
                            if (node.target && node.target.length > 0) {
                              commentBody += `   - ${node.target.join(' > ')}\n`;
                            }
                          }
                          if (violation.nodes.length > 5) {
                            commentBody += `   <details><summary><em>📋 View ${violation.nodes.length - 5} more affected elements...</em></summary>\n\n`;
                            for (const node of violation.nodes.slice(5)) {
                              if (node.target && node.target.length > 0) {
                                commentBody += `   - ${node.target.join(' > ')}\n`;
                              }
                            }
                            commentBody += `   </details>\n`;
                          }
                        }
                        commentBody += `\n</details>\n\n`;
                      }
                    }
                  }
                  
                  commentBody += `\n### 🛠️ Quick Fix Guide\n\n`;
                  commentBody += `- **Critical**: These break core functionality - fix immediately\n`;
                  commentBody += `- **Serious**: Major barriers for disabled users - high priority\n`;
                  commentBody += `- **Moderate**: Usability issues - medium priority\n`;
                  commentBody += `- **Minor**: Polish improvements - when time permits\n\n`;
                  commentBody += `📚 **Need Help?** Each violation above links to detailed WCAG documentation with examples.\n\n`;
                  commentBody += `</details>\n\n`;
                  console.log(`✅ Successfully generated ${violationsJson.length} violations organized by impact level`);
                } else {
                  console.log('⚠️ No violations JSON data available');
                  commentBody += `<details>\n<summary><strong>🔍 Detailed Analysis</strong> (Processing...)</summary>\n\n`;
                  commentBody += `### ⚠️ Detailed Analysis In Progress\n\n`;
                  commentBody += `The detailed breakdown is being processed. Meanwhile:\n\n`;
                  commentBody += `- **Download** the full accessibility report from workflow artifacts\n`;
                  commentBody += `- **Check** the workflow logs for detailed violation information\n`;
                  commentBody += `- **Focus** on the ${violationCount} total violations found\n\n`;
                  commentBody += `📊 **Status**: ${violationCount} violations detected, ${passesCount} checks passed\n\n`;
                  commentBody += `</details>\n\n`;
                }
              } catch (e) {
                console.log('❌ Could not process violations JSON:', e.message);
                commentBody += `### 🔍 Detailed Violation Breakdown\n\n`;
                commentBody += `⚠️ Error processing detailed breakdown: ${e.message}\n`;
                commentBody += `Download full accessibility report from workflow artifacts for complete analysis.\n\n`;
              }
              
              commentBody += `**💡 Action Required**: These violations prevent users with disabilities from accessing the site effectively.\n`;
              commentBody += `**🔧 Next Steps**: \n`;
              commentBody += `- Click on each violation type above for detailed remediation guidance\n`;
              commentBody += `- Use the provided WCAG documentation links for implementation details\n`;
              commentBody += `- Test with screen readers and keyboard navigation after fixes\n`;
              commentBody += `- Consider running accessibility tests locally during development\n\n`;
            } else if (hasAccessibilityResults === 'true' && violationCount === "0") {
              commentBody += `### ♿ Accessibility Status: EXCELLENT! 🎉\n\n`;
              commentBody += `**✅ No accessibility violations found!**\n\n`;
              commentBody += `| Result Type | Count | Status |\n`;
              commentBody += `|-------------|-------|--------|\n`;
              commentBody += `| ✅ Passes | ${passesCount} | WCAG 2.1 AA compliant |\n`;
              commentBody += `| ⚠️ Incomplete | ${incompleteCount} | Manual review recommended |\n\n`;
            }
            
            // Add meaningful test status summary (replaced generic table with rich content)
            
            commentBody += `🔗 **Alternative Testing**: [PageSpeed Insights](https://pagespeed.web.dev/analysis/${encodeURIComponent(targetUrl)})\n\n`;
            commentBody += `---\n*🤖 Multi-environment testing via reusable workflow - ${environmentName} environment*\n\n`;
            commentBody += `<!-- comprehensive-testing-${environmentName}-${Date.now()} -->`;
            
            // Atlantis-style comment management: Hide/minimize ALL previous comprehensive testing comments
            console.log(`🔍 Finding previous comprehensive testing comments for ${environmentName} environment...`);
            const { data: existingComments } = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: prNumber,
            });
            
            // Find all previous comprehensive testing comments for this environment
            const comprehensiveTestingComments = existingComments.filter(comment =>
              comment.body.includes(`comprehensive-testing-${environmentName}`) ||
              (comment.body.includes('🧪 Comprehensive Testing Results') && comment.body.includes(`Environment**: ${environmentName}`))
            );
            
            console.log(`🔍 Found ${comprehensiveTestingComments.length} previous comprehensive testing comments for ${environmentName}`);
            
            // Hide/minimize ALL previous comprehensive testing comments (Atlantis behavior) 
            let hiddenCount = 0;
            for (const oldComment of comprehensiveTestingComments) {
              try {
                // Add "outdated" marker to hide previous comments
                const outdatedBody = `<details>\n<summary>🔒 <strong>Outdated Testing Results (${environmentName})</strong> - Click to expand</summary>\n\n${oldComment.body}\n\n---\n*⚠️ This testing report has been superseded by a newer analysis.*\n</details>`;
                
                await github.rest.issues.updateComment({
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  comment_id: oldComment.id,
                  body: outdatedBody,
                });
                hiddenCount++;
              } catch (error) {
                console.log(`⚠️ Could not minimize testing comment ${oldComment.id}: ${error.message}`);
              }
            }
            
            console.log(`🔒 Minimized ${hiddenCount} previous comprehensive testing comments (Atlantis-style)`);
            
            // Always create fresh comment (never update existing)
            await github.rest.issues.createComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: prNumber,
              body: commentBody,
            });
            
            console.log(`✅ Posted fresh comprehensive testing results to PR #${prNumber} for ${environmentName} environment (${hiddenCount} old reports minimized)`);
            console.log(`🎯 Atlantis-style management: Only latest testing results visible, previous reports collapsed`);
